{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total models analyzed: 60\n",
      "Selection Strategy:\n",
      "  - Top 3: Validation RMSE + 1.0 × |Train RMSE - Val RMSE|\n",
      "  - Median: Overall RMSE = (Train RMSE + Val RMSE) / 2\n",
      "================================================================================\n",
      "TOP 3 MODELS (by Selection Score):\n",
      "==================================================\n",
      "1. model_14_20250910_234111 (seed 14):\n",
      "   Selection Score: 0.091584\n",
      "   Validation RMSE: 0.086629\n",
      "   Generalization Gap: 0.004956\n",
      "   Overall RMSE (train+val): 0.089106\n",
      "   Test RMSE: 0.107442\n",
      "\n",
      "2. model_260_20250910_210200 (seed 260):\n",
      "   Selection Score: 0.093481\n",
      "   Validation RMSE: 0.091056\n",
      "   Generalization Gap: 0.002424\n",
      "   Overall RMSE (train+val): 0.092268\n",
      "   Test RMSE: 0.114579\n",
      "\n",
      "3. model_212_20250910_195404 (seed 212):\n",
      "   Selection Score: 0.093577\n",
      "   Validation RMSE: 0.090630\n",
      "   Generalization Gap: 0.002946\n",
      "   Overall RMSE (train+val): 0.089157\n",
      "   Test RMSE: 0.112048\n",
      "\n",
      "================================================================================\n",
      "MEDIAN MODEL (by Overall RMSE - Rank 31/60):\n",
      "==================================================\n",
      "Model: model_32_20250911_014525 (seed 32)\n",
      "Overall RMSE (train+val): 0.105420\n",
      "Overall R² (train+val): 0.926112\n",
      "Train RMSE: 0.109757\n",
      "Validation RMSE: 0.101084\n",
      "Test RMSE: 0.128691\n",
      "Selection Score: 0.109757\n",
      "\n",
      "================================================================================\n",
      "PERFORMANCE STATISTICS (ALL MODELS)\n",
      "================================================================================\n",
      "\n",
      "ALL 60 MODELS SUMMARY:\n",
      "----------------------------------------\n",
      "Training     | R²: 0.9239±0.0135 | RMSE: 0.1074±0.0094\n",
      "Validation   | R²: 0.9281±0.0126 | RMSE: 0.1029±0.0089\n",
      "Test         | R²: 0.9031±0.0135 | RMSE: 0.1247±0.0086\n",
      "================================================================================\n",
      "SELECTED MODELS COMPARISON\n",
      "================================================================================\n",
      "\n",
      "TOP 3 MODELS:\n",
      "----------------------------------------\n",
      "Training     | R²: 0.9458±0.0035 | RMSE: 0.0909±0.0030\n",
      "Validation   | R²: 0.9460±0.0029 | RMSE: 0.0894±0.0024\n",
      "Test         | R²: 0.9230±0.0050 | RMSE: 0.1114±0.0036\n",
      "\n",
      "MEDIAN MODEL PERFORMANCE:\n",
      "----------------------------------------\n",
      "Training     | R²: 0.9211        | RMSE: 0.1098\n",
      "Validation   | R²: 0.9311        | RMSE: 0.1011\n",
      "Test         | R²: 0.8973        | RMSE: 0.1287\n",
      "\n",
      "================================================================================\n",
      "RETRIEVING MODEL WEIGHT FILES\n",
      "================================================================================\n",
      "\n",
      "TOP3:\n",
      "--------------------------------------------------\n",
      "✓ Found: model_14_20250910_234111.pth\n",
      "✓ Found: model_260_20250910_210200.pth\n",
      "✓ Found: model_212_20250910_195404.pth\n",
      "\n",
      "MEDIAN:\n",
      "--------------------------------------------------\n",
      "✓ Found: model_32_20250911_014525.pth\n",
      "\n",
      "SUMMARY: Found 4/4 weight files\n",
      "\n",
      "Copying weight files to selected_models/\n",
      "--------------------------------------------------\n",
      "✓ Copied: model_14_20250910_234111.pth -> rank1_top3_SRS_multirun_pinn_14.pth\n",
      "✓ Copied: model_260_20250910_210200.pth -> rank2_top3_SRS_multirun_pinn_260.pth\n",
      "✓ Copied: model_212_20250910_195404.pth -> rank3_top3_SRS_multirun_pinn_212.pth\n",
      "✓ Copied: model_32_20250911_014525.pth -> median_rank31_SRS_multirun_pinn_32.pth\n",
      "\n",
      "Detailed model info saved to: selected_models\\selected_models_detailed_info.csv\n",
      "Performance results saved to: selected_models\\selected_models_performance.csv\n",
      "\n",
      "================================================================================\n",
      "ALL FILES SAVED IN FOLDER: selected_models/\n",
      "================================================================================\n",
      "\n",
      "SELECTED MODELS vs ALL MODELS COMPARISON:\n",
      "--------------------------------------------------\n",
      "All 60 models avg selection score: 0.107548\n",
      "All 60 models avg overall RMSE: 0.105156\n",
      "Top 3 avg selection score: 0.092881\n",
      "Top 3 avg overall RMSE: 0.090177\n",
      "Median selection score: 0.109757\n",
      "Median overall RMSE: 0.105420\n",
      "\n",
      "Performance gaps from population mean:\n",
      "Top 3 selection score improvement: 13.6%\n",
      "Median vs population selection score: +2.1%\n",
      "\n",
      "Strategy comparison:\n",
      "- Top 3: Optimized for validation performance + low overfitting\n",
      "- Median: Represents typical train+val performance\n",
      "- Population stats show overall model quality and variability\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Selection based on validation RMSE + generalization gap (lambda = 1)\n",
    "selection_basis = 'Training-Validation Delta'\n",
    "lambda_val = 1.0\n",
    "n_top = 3\n",
    "# Setup paths\n",
    "runtyp = 'SRS_multirun_pinn'\n",
    "current_directory = os.getcwd()\n",
    "working_dir = os.path.dirname(current_directory)\n",
    "csv_directory = f\"{working_dir}/{runtyp}/metric_csv\"\n",
    "\n",
    "# Load all CSV files\n",
    "csv_files = glob.glob(os.path.join(csv_directory, \"*.csv\"))\n",
    "all_results = []\n",
    "\n",
    "for file_path in csv_files:\n",
    "    filename = os.path.basename(file_path)\n",
    "    parts = filename.replace('.csv', '').split('_')\n",
    "    \n",
    "    try:\n",
    "        seed = int(parts[1])\n",
    "        timestamp = parts[2] + '_' + parts[3]\n",
    "    except:\n",
    "        seed = None\n",
    "        timestamp = filename\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    df['filename'] = filename\n",
    "    df['seed'] = seed\n",
    "    df['model_id'] = f\"model_{seed}_{timestamp}\"\n",
    "    \n",
    "    all_results.append(df)\n",
    "\n",
    "# Combine all results\n",
    "combined_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "# Calculate scores for each model\n",
    "model_scores = []\n",
    "\n",
    "for model_id in combined_df['model_id'].unique():\n",
    "    model_data = combined_df[combined_df['model_id'] == model_id]\n",
    "    \n",
    "    # Get metrics for all datasets\n",
    "    train_rmse = model_data[model_data['Dataset'] == 'Training']['RMSE'].iloc[0]\n",
    "    train_r2 = model_data[model_data['Dataset'] == 'Training']['R2'].iloc[0]\n",
    "    val_rmse = model_data[model_data['Dataset'] == 'Validation']['RMSE'].iloc[0]\n",
    "    val_r2 = model_data[model_data['Dataset'] == 'Validation']['R2'].iloc[0]\n",
    "    test_rmse = model_data[model_data['Dataset'] == 'Test']['RMSE'].iloc[0]\n",
    "    test_r2 = model_data[model_data['Dataset'] == 'Test']['R2'].iloc[0]\n",
    "    \n",
    "    # Calculate scores\n",
    "    generalization_gap = abs(train_rmse - val_rmse)\n",
    "    selection_score = val_rmse + lambda_val * generalization_gap  # For top N\n",
    "    overall_rmse = (train_rmse + val_rmse) / 2  # For median (no test set)\n",
    "    overall_r2 = (train_r2 + val_r2) / 2\n",
    "    \n",
    "    model_scores.append({\n",
    "        'model_id': model_id,\n",
    "        'seed': model_data['seed'].iloc[0],\n",
    "        'train_rmse': train_rmse,\n",
    "        'train_r2': train_r2,\n",
    "        'val_rmse': val_rmse,\n",
    "        'val_r2': val_r2,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_r2': test_r2,\n",
    "        'generalization_gap': generalization_gap,\n",
    "        'selection_score': selection_score,\n",
    "        'overall_rmse': overall_rmse,\n",
    "        'overall_r2': overall_r2\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "scores_df = pd.DataFrame(model_scores)\n",
    "\n",
    "print(f\"Total models analyzed: {len(scores_df)}\")\n",
    "print(f\"Selection Strategy:\")\n",
    "print(f\"  - Top {n_top}: Validation RMSE + {lambda_val} × |Train RMSE - Val RMSE|\")\n",
    "print(f\"  - Median: Overall RMSE = (Train RMSE + Val RMSE) / 2\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# GET TOP N MODELS (by selection score)\n",
    "top_scores = scores_df.nsmallest(n_top, 'selection_score').reset_index(drop=True)\n",
    "top_n_models = top_scores['model_id'].tolist()\n",
    "\n",
    "print(f\"TOP {n_top} MODELS (by Selection Score):\")\n",
    "print(\"=\"*50)\n",
    "for i, (_, row) in enumerate(top_scores.iterrows(), 1):\n",
    "    print(f\"{i}. {row['model_id']} (seed {row['seed']}):\")\n",
    "    print(f\"   Selection Score: {row['selection_score']:.6f}\")\n",
    "    print(f\"   Validation RMSE: {row['val_rmse']:.6f}\")\n",
    "    print(f\"   Generalization Gap: {row['generalization_gap']:.6f}\")\n",
    "    print(f\"   Overall RMSE (train+val): {row['overall_rmse']:.6f}\")\n",
    "    print(f\"   Test RMSE: {row['test_rmse']:.6f}\")\n",
    "    print()\n",
    "\n",
    "# GET MEDIAN MODEL (by overall RMSE)\n",
    "median_scores = scores_df.sort_values('overall_rmse').reset_index(drop=True)\n",
    "total_models = len(scores_df)\n",
    "median_rank = total_models // 2  # For 20 models, this gives index 10 (rank 11)\n",
    "median_model_info = median_scores.iloc[median_rank]\n",
    "median_model_id = median_model_info['model_id']\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"MEDIAN MODEL (by Overall RMSE - Rank {median_rank+1}/{total_models}):\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Model: {median_model_info['model_id']} (seed {median_model_info['seed']})\")\n",
    "print(f\"Overall RMSE (train+val): {median_model_info['overall_rmse']:.6f}\")\n",
    "print(f\"Overall R² (train+val): {median_model_info['overall_r2']:.6f}\")\n",
    "print(f\"Train RMSE: {median_model_info['train_rmse']:.6f}\")\n",
    "print(f\"Validation RMSE: {median_model_info['val_rmse']:.6f}\")\n",
    "print(f\"Test RMSE: {median_model_info['test_rmse']:.6f}\")\n",
    "print(f\"Selection Score: {median_model_info['selection_score']:.6f}\")\n",
    "print()\n",
    "\n",
    "# Combine selected models\n",
    "all_selected_models = top_n_models + [median_model_id]\n",
    "all_selected_models = list(set(all_selected_models))  # Remove duplicates\n",
    "\n",
    "# Performance summaries\n",
    "selected_combined_df = combined_df[combined_df['model_id'].isin(all_selected_models)]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PERFORMANCE STATISTICS (ALL MODELS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# ALL MODELS summary\n",
    "print(f\"\\nALL {total_models} MODELS SUMMARY:\")\n",
    "print(\"-\" * 40)\n",
    "for dataset in ['Training', 'Validation', 'Test']:\n",
    "    subset = combined_df[combined_df['Dataset'] == dataset]\n",
    "    r2_mean = subset['R2'].mean()\n",
    "    r2_std = subset['R2'].std()\n",
    "    rmse_mean = subset['RMSE'].mean()\n",
    "    rmse_std = subset['RMSE'].std()\n",
    "    print(f\"{dataset:12} | R²: {r2_mean:.4f}±{r2_std:.4f} | RMSE: {rmse_mean:.4f}±{rmse_std:.4f}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SELECTED MODELS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Top N summary\n",
    "top_n_subset = combined_df[combined_df['model_id'].isin(top_n_models)]\n",
    "print(f\"\\nTOP {n_top} MODELS:\")\n",
    "print(\"-\" * 40)\n",
    "for dataset in ['Training', 'Validation', 'Test']:\n",
    "    subset = top_n_subset[top_n_subset['Dataset'] == dataset]\n",
    "    r2_mean = subset['R2'].mean()\n",
    "    r2_std = subset['R2'].std()\n",
    "    rmse_mean = subset['RMSE'].mean()\n",
    "    rmse_std = subset['RMSE'].std()\n",
    "    print(f\"{dataset:12} | R²: {r2_mean:.4f}±{r2_std:.4f} | RMSE: {rmse_mean:.4f}±{rmse_std:.4f}\")\n",
    "\n",
    "# Median model summary\n",
    "median_subset = combined_df[combined_df['model_id'] == median_model_id]\n",
    "print(f\"\\nMEDIAN MODEL PERFORMANCE:\")\n",
    "print(\"-\" * 40)\n",
    "for dataset in ['Training', 'Validation', 'Test']:\n",
    "    subset = median_subset[median_subset['Dataset'] == dataset]\n",
    "    if len(subset) > 0:\n",
    "        r2_val = subset['R2'].iloc[0]\n",
    "        rmse_val = subset['RMSE'].iloc[0]\n",
    "        print(f\"{dataset:12} | R²: {r2_val:.4f}        | RMSE: {rmse_val:.4f}\")\n",
    "\n",
    "# Handle model weights\n",
    "model_weights_dir = os.path.join(working_dir, runtyp, \"model_weights\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RETRIEVING MODEL WEIGHT FILES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def find_weights(model_ids, category_name):\n",
    "    found_weights = []\n",
    "    missing_weights = []\n",
    "    \n",
    "    print(f\"\\n{category_name.upper()}:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for model_id in model_ids:\n",
    "        parts = model_id.split('_')\n",
    "        seed = parts[1]\n",
    "        timestamp = parts[2] + '_' + parts[3]\n",
    "        \n",
    "        weight_filename = f\"model_{seed}_{timestamp}.pth\"\n",
    "        weight_filepath = os.path.join(model_weights_dir, weight_filename)\n",
    "        \n",
    "        if os.path.exists(weight_filepath):\n",
    "            found_weights.append({\n",
    "                'model_id': model_id,\n",
    "                'seed': seed,\n",
    "                'weight_file': weight_filename,\n",
    "                'weight_path': weight_filepath,\n",
    "                'category': category_name\n",
    "            })\n",
    "            print(f\"✓ Found: {weight_filename}\")\n",
    "        else:\n",
    "            missing_weights.append({\n",
    "                'model_id': model_id,\n",
    "                'seed': seed,\n",
    "                'expected_file': weight_filename,\n",
    "                'category': category_name\n",
    "            })\n",
    "            print(f\"✗ Missing: {weight_filename}\")\n",
    "    \n",
    "    return found_weights, missing_weights\n",
    "\n",
    "# Find weights\n",
    "top_n_weights, top_n_missing = find_weights(top_n_models, f\"top{n_top}\")\n",
    "median_weights, median_missing = find_weights([median_model_id], \"median\")\n",
    "\n",
    "all_found_weights = top_n_weights + median_weights\n",
    "all_missing_weights = top_n_missing + median_missing\n",
    "\n",
    "print(f\"\\nSUMMARY: Found {len(all_found_weights)}/{len(all_selected_models)} weight files\")\n",
    "\n",
    "# Copy weight files\n",
    "output_folder = 'selected_models'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "print(f\"\\nCopying weight files to {output_folder}/\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for weight_info in all_found_weights:\n",
    "    src_path = weight_info['weight_path']\n",
    "    \n",
    "    if weight_info['category'] == f'top{n_top}':\n",
    "        model_rank = top_n_models.index(weight_info['model_id']) + 1\n",
    "        new_filename = f\"rank{model_rank}_top{n_top}_{runtyp}_{weight_info['seed']}.pth\"\n",
    "    else:  # median\n",
    "        new_filename = f\"median_rank{median_rank+1}_{runtyp}_{weight_info['seed']}.pth\"\n",
    "    \n",
    "    dst_path = os.path.join(output_folder, new_filename)\n",
    "    shutil.copy2(src_path, dst_path)\n",
    "    print(f\"✓ Copied: {weight_info['weight_file']} -> {new_filename}\")\n",
    "    weight_info['renamed_file'] = new_filename\n",
    "\n",
    "# Save detailed information\n",
    "if all_found_weights:\n",
    "    detailed_info = []\n",
    "    for weight in all_found_weights:\n",
    "        model_id = weight['model_id']\n",
    "        score_info = scores_df[scores_df['model_id'] == model_id].iloc[0]\n",
    "        \n",
    "        combined_info = {\n",
    "            **weight,\n",
    "            'train_rmse': score_info['train_rmse'],\n",
    "            'val_rmse': score_info['val_rmse'],\n",
    "            'test_rmse': score_info['test_rmse'],\n",
    "            'generalization_gap': score_info['generalization_gap'],\n",
    "            'selection_score': score_info['selection_score'],\n",
    "            'overall_rmse': score_info['overall_rmse'],\n",
    "            'overall_r2': score_info['overall_r2']\n",
    "        }\n",
    "        detailed_info.append(combined_info)\n",
    "    \n",
    "    detailed_df = pd.DataFrame(detailed_info)\n",
    "    weight_info_path = os.path.join(output_folder, 'selected_models_detailed_info.csv')\n",
    "    detailed_df.to_csv(weight_info_path, index=False)\n",
    "    print(f\"\\nDetailed model info saved to: {weight_info_path}\")\n",
    "\n",
    "# Save performance results\n",
    "results_wide = []\n",
    "for model_id in all_selected_models:\n",
    "    if model_id in [w['model_id'] for w in all_found_weights]:\n",
    "        model_data = combined_df[combined_df['model_id'] == model_id]\n",
    "        score_data = scores_df[scores_df['model_id'] == model_id].iloc[0]\n",
    "        \n",
    "        if model_id in top_n_models:\n",
    "            category = f\"top{n_top}\"\n",
    "            rank_in_category = top_n_models.index(model_id) + 1\n",
    "        else:\n",
    "            category = \"median\"\n",
    "            rank_in_category = median_rank + 1\n",
    "        \n",
    "        train_data = model_data[model_data['Dataset'] == 'Training']\n",
    "        val_data = model_data[model_data['Dataset'] == 'Validation']\n",
    "        test_data = model_data[model_data['Dataset'] == 'Test']\n",
    "        \n",
    "        row = {\n",
    "            'category': category,\n",
    "            'rank_in_category': rank_in_category,\n",
    "            'model_id': model_id,\n",
    "            'seed': model_data['seed'].iloc[0],\n",
    "            'training_r2': train_data['R2'].iloc[0],\n",
    "            'training_rmse': train_data['RMSE'].iloc[0],\n",
    "            'val_r2': val_data['R2'].iloc[0],\n",
    "            'val_rmse': val_data['RMSE'].iloc[0],\n",
    "            'test_r2': test_data['R2'].iloc[0],\n",
    "            'test_rmse': test_data['RMSE'].iloc[0],\n",
    "            'generalization_gap': score_data['generalization_gap'],\n",
    "            'selection_score': score_data['selection_score'],\n",
    "            'overall_rmse': score_data['overall_rmse'],\n",
    "            'overall_r2': score_data['overall_r2'],\n",
    "            'filename': model_data['filename'].iloc[0]\n",
    "        }\n",
    "        results_wide.append(row)\n",
    "\n",
    "if results_wide:\n",
    "    results_wide_df = pd.DataFrame(results_wide)\n",
    "    results_wide_df = results_wide_df.sort_values(['category', 'rank_in_category'])\n",
    "    results_path = os.path.join(output_folder, 'selected_models_performance.csv')\n",
    "    results_wide_df.to_csv(results_path, index=False)\n",
    "    print(f\"Performance results saved to: {results_path}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"ALL FILES SAVED IN FOLDER: {output_folder}/\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Final comparison\n",
    "print(f\"\\nSELECTED MODELS vs ALL MODELS COMPARISON:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# All models stats\n",
    "all_models_selection_mean = scores_df['selection_score'].mean()\n",
    "all_models_overall_mean = scores_df['overall_rmse'].mean()\n",
    "\n",
    "# Top N stats  \n",
    "top_n_avg_selection = top_scores['selection_score'].mean()\n",
    "top_n_avg_overall = top_scores['overall_rmse'].mean()\n",
    "\n",
    "print(f\"All {total_models} models avg selection score: {all_models_selection_mean:.6f}\")\n",
    "print(f\"All {total_models} models avg overall RMSE: {all_models_overall_mean:.6f}\")\n",
    "print(f\"Top {n_top} avg selection score: {top_n_avg_selection:.6f}\")\n",
    "print(f\"Top {n_top} avg overall RMSE: {top_n_avg_overall:.6f}\")\n",
    "print(f\"Median selection score: {median_model_info['selection_score']:.6f}\")\n",
    "print(f\"Median overall RMSE: {median_model_info['overall_rmse']:.6f}\")\n",
    "\n",
    "print(f\"\\nPerformance gaps from population mean:\")\n",
    "top_n_selection_gap = ((all_models_selection_mean - top_n_avg_selection) / all_models_selection_mean) * 100\n",
    "median_selection_gap = ((median_model_info['selection_score'] - all_models_selection_mean) / all_models_selection_mean) * 100\n",
    "\n",
    "print(f\"Top {n_top} selection score improvement: {top_n_selection_gap:.1f}%\")\n",
    "print(f\"Median vs population selection score: {median_selection_gap:+.1f}%\")\n",
    "\n",
    "print(f\"\\nStrategy comparison:\")\n",
    "print(f\"- Top {n_top}: Optimized for validation performance + low overfitting\")\n",
    "print(f\"- Median: Represents typical train+val performance\")\n",
    "print(f\"- Population stats show overall model quality and variability\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
