{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORKSPACE SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETER SETTINGS\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "use_physics_in_loss=True \n",
    "monitor_physics=True\n",
    "# Reproducibility settings\n",
    "seed = seed\n",
    "split_seed = 42\n",
    "# Hyperparameters\n",
    "hidden_dim=64\n",
    "graph_layers=2\n",
    "fc_layers=3\n",
    "lr = 0.0015725630394219055\n",
    "weight_decay = 3.974062330707054e-06\n",
    "num_epochs = 200\n",
    "batch_size = 32\n",
    "runtime = timestamp\n",
    "runtyp = 'RASHYB_multirun_pinn'\n",
    "# Thermo loss strength\n",
    "s1 = 10\n",
    "s2 = 1000\n",
    "s3 = 1\n",
    "\n",
    "print('seed             :', seed)\n",
    "print('split seed       :', split_seed)\n",
    "print('hidden_dim       :', hidden_dim)\n",
    "print('lr               :', lr)\n",
    "print('decay            :', weight_decay)\n",
    "print('batch_size       :', batch_size)\n",
    "print('max_epochs       :', num_epochs)\n",
    "print('gradP strength   :', s1)\n",
    "print('gradT strength   :', s2)\n",
    "print('gradC strength   :', s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING LIBRARIES\n",
    "import copy\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'  # or ':16:8'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdmolops\n",
    "from rdkit.Chem import rdDistGeom as molDG\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem.rdchem import GetPeriodicTable\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch_geometric.nn import MessagePassing, GCNConv, global_mean_pool, GATConv\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# FUNCTIONS\n",
    "from data_processing import load_dataset, process_dataset\n",
    "from path_helpers import get_path\n",
    "from stats_compute import compute_statistics, scale_graphs\n",
    "import ModelArchitecture\n",
    "from EnhancedDataSplit import DataSplitter\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import Tuple, List\n",
    "\n",
    "# DIRECTORY SETUP\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA Deterministic (ON/OFF SETTING)\n",
    "# For PyTorch\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.use_deterministic_algorithms(False)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "selected_device = 'cuda' # either 'cuda' or 'cpu\n",
    "device = torch.device(selected_device)\n",
    "print('device           :', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION -> TRAINING LOOP\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, \n",
    "                    pco2_std, temp_std, conc_std,\n",
    "                    pco2_mean, temp_mean, conc_mean,\n",
    "                    s1, s2, s3, \n",
    "                    use_physics_in_loss=True, monitor_physics=True):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_main_loss = 0\n",
    "    total_thermo1_loss = 0\n",
    "    total_thermo2_loss = 0\n",
    "    total_thermo3_loss = 0\n",
    "    \n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        loss_main = criterion(output, data.aco2.view(-1, 1))\n",
    "        \n",
    "        # Calculate physics losses\n",
    "        if use_physics_in_loss or monitor_physics:\n",
    "            loss_thermo1 = ModelArchitecture.grad_pres(model, data, pco2_std, pco2_mean, s1) if s1 != 0 else 0.0\n",
    "            loss_thermo2 = ModelArchitecture.grad_temp(model, data, temp_std, temp_mean, s2) if s2 != 0 else 0.0\n",
    "            loss_thermo3 = ModelArchitecture.grad_conc(model, data, conc_std, conc_mean, s3) if s3 != 0 else 0.0\n",
    "        else:\n",
    "            loss_thermo1 = loss_thermo2 = loss_thermo3 = 0.0\n",
    "        \n",
    "        # Only include physics losses in backprop if use_physics_in_loss=True\n",
    "        if use_physics_in_loss:\n",
    "            loss = loss_main + loss_thermo1 + loss_thermo2 + loss_thermo3\n",
    "        else:\n",
    "            loss = loss_main\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track total loss consistently with what was actually backpropagated\n",
    "        if use_physics_in_loss:\n",
    "            total_loss += loss.item()  # This includes physics losses\n",
    "        else:\n",
    "            total_loss += loss_main.item()\n",
    "        total_main_loss += loss_main.item()\n",
    "        total_thermo1_loss += loss_thermo1.item() if isinstance(loss_thermo1, torch.Tensor) else loss_thermo1\n",
    "        total_thermo2_loss += loss_thermo2.item() if isinstance(loss_thermo2, torch.Tensor) else loss_thermo2\n",
    "        total_thermo3_loss += loss_thermo3.item() if isinstance(loss_thermo3, torch.Tensor) else loss_thermo3\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    avg_main_loss = total_main_loss / len(train_loader)\n",
    "    avg_thermo1_loss = total_thermo1_loss / len(train_loader)\n",
    "    avg_thermo2_loss = total_thermo2_loss / len(train_loader)\n",
    "    avg_thermo3_loss = total_thermo3_loss / len(train_loader)\n",
    "    \n",
    "    return avg_train_loss, avg_main_loss, avg_thermo1_loss, avg_thermo2_loss, avg_thermo3_loss\n",
    "\n",
    "\n",
    "# Validation Step\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data.aco2.view(-1, 1))\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    return avg_val_loss\n",
    "\n",
    "# Test Evaluation (Metrics)\n",
    "def evaluate_test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_outputs = []\n",
    "    test_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data.aco2.view(-1, 1))\n",
    "            test_loss += loss.item()\n",
    "            test_outputs.append(output.cpu().numpy())\n",
    "            test_targets.append(data.aco2.view(-1, 1).cpu().numpy())\n",
    "    \n",
    "    test_outputs = np.concatenate(test_outputs, axis=0)\n",
    "    test_targets = np.concatenate(test_targets, axis=0)\n",
    "    test_r2 = r2_score(test_targets, test_outputs)\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    return avg_test_loss, test_r2\n",
    "\n",
    "# Loss Plotting\n",
    "def plot_losses(train_losses, val_losses, test_losses, test_r2_scores, \n",
    "                main_losses, thermo1_losses, thermo2_losses, thermo3_losses, \n",
    "                stopped_epoch=None):\n",
    "    # Original combined loss plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Total Train Loss')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "    plt.plot(range(1, len(test_losses) + 1), test_losses, label='Test Loss')\n",
    "    if stopped_epoch:\n",
    "        plt.axvline(x=stopped_epoch, color='red', linestyle='--', alpha=0.7, label=f'Early Stop (Epoch {stopped_epoch})')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training, Validation and Test Loss Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(range(1, len(test_r2_scores) + 1), test_r2_scores, color='green')\n",
    "    if stopped_epoch:\n",
    "        plt.axvline(x=stopped_epoch, color='red', linestyle='--', alpha=0.7, label=f'Early Stop (Epoch {stopped_epoch})')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('R² Score')\n",
    "    plt.title('Test R² Score per Epoch')\n",
    "    plt.legend() if stopped_epoch else None\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Individual physics loss components\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(range(1, len(main_losses) + 1), main_losses, label='Main Loss', color='blue')\n",
    "    plt.plot(range(1, len(thermo1_losses) + 1), thermo1_losses, label='Partial pressure gradient (s1)', color='red')\n",
    "    plt.plot(range(1, len(thermo2_losses) + 1), thermo2_losses, label='Temperature gradient (s2)', color='orange')\n",
    "    plt.plot(range(1, len(thermo3_losses) + 1), thermo3_losses, label='Concentration gradient (s3)', color='purple')\n",
    "    if stopped_epoch:\n",
    "        plt.axvline(x=stopped_epoch, color='red', linestyle='--', alpha=0.7)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Individual Loss Components')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Physics losses only (zoomed view)\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(range(1, len(thermo1_losses) + 1), thermo1_losses, label='Partial pressure gradient (s1)', color='red')\n",
    "    plt.plot(range(1, len(thermo2_losses) + 1), thermo2_losses, label='Temperature gradient (s2)', color='orange')\n",
    "    plt.plot(range(1, len(thermo3_losses) + 1), thermo3_losses, label='Concentration gradient (s3)', color='purple')\n",
    "    if stopped_epoch:\n",
    "        plt.axvline(x=stopped_epoch, color='red', linestyle='--', alpha=0.7)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Physics Loss Components (Zoomed)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main Training Loop with Early Stopping and Best Weight Restoration\n",
    "def train_model(model, train_loader, val_loader, \n",
    "                test_loader, criterion, optimizer, \n",
    "                pco2_std, temp_std, conc_std,\n",
    "                pco2_mean, temp_mean, conc_mean,\n",
    "                s1, s2, s3,\n",
    "                device, num_epochs, \n",
    "                seed, parent_directory,\n",
    "                use_physics_in_loss=True, monitor_physics=True,\n",
    "                patience=10, min_delta=1e-5):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        use_physics_in_loss (bool): Whether to include physics losses in backpropagation\n",
    "        monitor_physics (bool): Whether to compute and track physics losses for monitoring\n",
    "        patience (int): Number of epochs to wait for improvement before stopping\n",
    "        min_delta (float): Minimum change in validation loss to qualify as improvement\n",
    "    \"\"\"\n",
    "    # Initialize loss tracking lists\n",
    "    train_losses, val_losses, test_losses, test_r2_scores = [], [], [], []\n",
    "    main_losses, thermo1_losses, thermo2_losses, thermo3_losses = [], [], [], []\n",
    "\n",
    "    # Early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_weights = None  # Store best model weights in memory\n",
    "    best_epoch = 0\n",
    "    epochs_no_improve = 0\n",
    "    stopped_epoch = None\n",
    "\n",
    "    # Print training mode\n",
    "    if use_physics_in_loss and monitor_physics:\n",
    "        print(\"Training with physics losses in backpropagation + monitoring\")\n",
    "    elif monitor_physics and not use_physics_in_loss:\n",
    "        print(\"Training with Main Loss only, but monitoring physics losses\")\n",
    "    elif use_physics_in_loss and not monitor_physics:\n",
    "        print(\"Training with physics losses in backpropagation (no monitoring)\")\n",
    "    else:\n",
    "        print(\"Training with Main Loss only (no physics)\")\n",
    "    \n",
    "    print(f\"Early stopping enabled: patience={patience}, min_delta={min_delta}\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Get individual loss components from training\n",
    "        avg_train_loss, avg_main_loss, avg_thermo1_loss, avg_thermo2_loss, avg_thermo3_loss = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, \n",
    "            pco2_std, temp_std, conc_std, \n",
    "            pco2_mean, temp_mean, conc_mean,\n",
    "            s1, s2, s3, use_physics_in_loss, monitor_physics)\n",
    "\n",
    "        avg_val_loss = validate(model, val_loader, criterion, device)\n",
    "        avg_test_loss, test_r2 = evaluate_test(model, test_loader, criterion, device)\n",
    "\n",
    "        # Store all losses\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        test_r2_scores.append(test_r2)\n",
    "        main_losses.append(avg_main_loss)\n",
    "        thermo1_losses.append(avg_thermo1_loss)\n",
    "        thermo2_losses.append(avg_thermo2_loss)\n",
    "        thermo3_losses.append(avg_thermo3_loss)\n",
    "\n",
    "        # Early stopping check and best model tracking\n",
    "        if avg_val_loss < (best_val_loss - min_delta):\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_epoch = epoch + 1\n",
    "            epochs_no_improve = 0\n",
    "            # Store the best model weights in memory (deep copy)\n",
    "            best_model_weights = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        # Enhanced print statement with individual loss components\n",
    "        if monitor_physics:\n",
    "            physics_info = f'Main Loss: {round(avg_main_loss, 4)}, gradP: {round(avg_thermo1_loss, 4)}, gradT: {round(avg_thermo2_loss, 4)}, gradC: {round(avg_thermo3_loss, 4)}'\n",
    "            physics_status = \" [BACKPROP]\" if use_physics_in_loss else \" [MONITOR]\"\n",
    "        else:\n",
    "            physics_info = f'Main Loss: {round(avg_main_loss, 4)}'\n",
    "            physics_status = \"\"\n",
    "            \n",
    "        early_stop_info = f\" (No improve: {epochs_no_improve}/{patience})\" if epochs_no_improve > 0 else \"\"\n",
    "        best_marker = \" *BEST*\" if epoch + 1 == best_epoch else \"\"\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Total Loss: {round(avg_train_loss, 4)}, '\n",
    "              f'Val Loss: {round(avg_val_loss, 4)}, Test Loss: {round(avg_test_loss, 4)}, '\n",
    "              f'Test R2: {round(test_r2, 4)} | {physics_info}{physics_status}{early_stop_info}{best_marker}')\n",
    "        \n",
    "        # Check if we should stop early\n",
    "        if epochs_no_improve >= patience:\n",
    "            stopped_epoch = epoch + 1\n",
    "            print(f\"\\nEarly stopping triggered at epoch {stopped_epoch}\")\n",
    "            print(f\"Best validation loss: {round(best_val_loss, 6)} at epoch {best_epoch}\")\n",
    "            break\n",
    "    \n",
    "    # Restore the best model weights\n",
    "    if best_model_weights is not None:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "        print(f\"\\nRestored model weights from epoch {best_epoch} (best validation loss: {round(best_val_loss, 6)})\")\n",
    "    else:\n",
    "        print(\"\\nWarning: No improvement found during training, keeping final weights\")\n",
    "    \n",
    "    # Final summary\n",
    "    if stopped_epoch is None:\n",
    "        print(f\"Training completed all {num_epochs} epochs\")\n",
    "    else:\n",
    "        print(f\"Training stopped early at epoch {stopped_epoch}/{num_epochs}\")\n",
    "    \n",
    "    print(f\"Model now contains weights from epoch {best_epoch}\")\n",
    "    print(f\"Final test R² with best weights: {round(test_r2_scores[best_epoch-1], 6)}\")\n",
    "    \n",
    "    # Plot all losses including individual components\n",
    "    plot_losses(train_losses, val_losses, test_losses, test_r2_scores, \n",
    "                main_losses, thermo1_losses, thermo2_losses, thermo3_losses,\n",
    "                stopped_epoch)\n",
    "    \n",
    "    # Return all tracked losses and stopping info, plus best epoch info\n",
    "    return (train_losses, val_losses, test_losses, test_r2_scores, \n",
    "            main_losses, thermo1_losses, thermo2_losses, thermo3_losses,\n",
    "            stopped_epoch, best_val_loss, best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD & GRAPH GENERATION\n",
    "df_components = load_dataset(get_path(file_name = 'components_set.csv', folder_name='datasets'))\n",
    "smiles_dict = dict(zip(df_components['Abbreviation'], df_components['SMILES']))\n",
    "df_systems = load_dataset(get_path(file_name = 'systems_set.csv', folder_name='datasets'))\n",
    "smiles_list = df_components[\"SMILES\"].dropna().tolist()\n",
    "mol_name_dict = smiles_dict.copy()\n",
    "# GRAPH\n",
    "system_graphs = process_dataset(df_systems, smiles_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATASET\n",
    "splitter_1 = DataSplitter(system_graphs, random_state=split_seed)\n",
    "RASset1, RASset2, RASset3 = splitter_1.rarity_aware_unseen_amine_split()\n",
    "opt_data = RASset1 + RASset2\n",
    "\n",
    "# HYBRID\n",
    "splitter_2 = DataSplitter(opt_data, random_state=split_seed)\n",
    "SRSset1, SRSset2, SRSset3 = splitter_2.stratified_random_split()\n",
    "train_data = SRSset1\n",
    "val_data = SRSset2 + SRSset3\n",
    "test_data = RASset3\n",
    "#Retrieve the statistics of train_data\n",
    "stats = compute_statistics(train_data)\n",
    "conc_mean = stats[0]\n",
    "conc_std = stats[1]\n",
    "temp_mean = stats[2]\n",
    "temp_std = stats[3]\n",
    "pco2_mean = stats[4]\n",
    "pco2_std = stats[5]\n",
    "#Apply the scaling to validation and test\n",
    "original_train_data = copy.deepcopy(train_data)\n",
    "original_val_data = copy.deepcopy(val_data)\n",
    "original_test_data = copy.deepcopy(test_data)\n",
    "combined_original_data = original_train_data + original_val_data + original_test_data\n",
    "train_data = scale_graphs(train_data, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    "val_data = scale_graphs(val_data, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    "test_data = scale_graphs(test_data, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    "#Load the data into DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN WITH OPTIMIZED HYPERPARAMETERS\n",
    "print(\"Training final model with optimized hyperparameters...\")\n",
    "\n",
    "# Load the data into DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "node_dim = train_data[0].x.size(1)\n",
    "edge_dim = train_data[0].edge_attr.size(1)\n",
    "\n",
    "model = ModelArchitecture.VLEAmineCO2(node_dim=node_dim,\n",
    "                    edge_dim=edge_dim, \n",
    "                    hidden_dim=hidden_dim,\n",
    "                    graph_layers=graph_layers,\n",
    "                    fc_layers=fc_layers,\n",
    "                    use_adaptive_pooling=True\n",
    "                    ).to(device)\n",
    "\n",
    "criterion = ModelArchitecture.MSLELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# Start timing\n",
    "real_time_start = time.time()  # Real time (wall-clock time)\n",
    "cpu_time_start = time.process_time()  # CPU time\n",
    "\n",
    "# Updated to receive all loss components\n",
    "(train_losses, val_losses, test_losses, test_r2_scores, \n",
    " main_losses, thermo1_losses, thermo2_losses, thermo3_losses,\n",
    " stopped_epoch, best_val_loss, best_epoch) = train_model(\n",
    "    model, train_loader, val_loader, test_loader, \n",
    "    criterion, optimizer, \n",
    "    pco2_std, temp_std, conc_std, \n",
    "    pco2_mean, temp_mean, conc_mean,\n",
    "    s1, s2, s3, \n",
    "    device, num_epochs=num_epochs,\n",
    "    seed=seed, parent_directory=parent_directory,\n",
    "    use_physics_in_loss=use_physics_in_loss, \n",
    "    monitor_physics=monitor_physics,\n",
    "    patience=10, min_delta=1e-6)\n",
    "\n",
    "model_path = f\"{parent_directory}/models/models_root/{runtyp}/model_weights/MODEL_{seed}_{runtime}.pth\"\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "torch.save(model.state_dict(), model_path)\n",
    "# Enhanced CSV path with individual loss tracking\n",
    "csv_path = f\"{parent_directory}/models/models_root/{runtyp}/losses/losses_{seed}_{runtime}.csv\"\n",
    "os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "\n",
    "# Write comprehensive loss data to CSV\n",
    "with open(csv_path, mode='w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    # Write enhanced header with individual loss components\n",
    "    writer.writerow([\n",
    "        \"Epoch\", \"Train Loss\", \"Validation Loss\", \"Test Loss\", \"Test R2\",\n",
    "        \"Main Loss\", \"gradP Loss (s1)\", \"gradT Loss (s2)\", \"gradC Loss (s3)\"\n",
    "    ])\n",
    "    \n",
    "    # Write losses for each epoch including individual components (training)\n",
    "    for epoch in range(len(train_losses)):\n",
    "        writer.writerow([\n",
    "            epoch + 1,\n",
    "            train_losses[epoch],\n",
    "            val_losses[epoch],\n",
    "            test_losses[epoch],\n",
    "            test_r2_scores[epoch],\n",
    "            main_losses[epoch],\n",
    "            thermo1_losses[epoch],\n",
    "            thermo2_losses[epoch],\n",
    "            thermo3_losses[epoch]\n",
    "        ])\n",
    "\n",
    "# Optional: Save individual physics loss components to separate CSV for detailed analysis\n",
    "physics_csv_path = f\"{parent_directory}/models/models_root/{runtyp}/physics_loss/physics_losses_{seed}_{runtime}.csv\"\n",
    "os.makedirs(os.path.dirname(physics_csv_path), exist_ok=True)\n",
    "with open(physics_csv_path, mode='w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Epoch\", \"Main Loss\", \"gradP Loss (s1)\", \"gradT Loss (s2)\", \"gradC Loss (s3)\", \"Physics Loss Sum\"])\n",
    "    \n",
    "    for epoch in range(len(main_losses)):\n",
    "        physics_sum = thermo1_losses[epoch] + thermo2_losses[epoch] + thermo3_losses[epoch]\n",
    "        writer.writerow([\n",
    "            epoch + 1,\n",
    "            main_losses[epoch],\n",
    "            thermo1_losses[epoch],\n",
    "            thermo2_losses[epoch],\n",
    "            thermo3_losses[epoch],\n",
    "            physics_sum\n",
    "        ])\n",
    "\n",
    "# End timing\n",
    "real_time_end = time.time()\n",
    "cpu_time_end = time.process_time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "real_time_elapsed = real_time_end - real_time_start\n",
    "cpu_time_elapsed = cpu_time_end - cpu_time_start\n",
    "\n",
    "# Output the training time\n",
    "print(f\"Training Real Time (Wall-Clock Time): {real_time_elapsed:.2f} seconds\")\n",
    "print(f\"Training CPU Time: {cpu_time_elapsed:.2f} seconds\")\n",
    "\n",
    "# Print summary statistics of loss components\n",
    "print(\"\\n=== Training Summary ===\")\n",
    "print(f\"Final Total Train Loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"Final Validation Loss: {val_losses[-1]:.6f}\")\n",
    "print(f\"Final Test Loss: {test_losses[-1]:.6f}\")\n",
    "print(f\"Final Test R2: {test_r2_scores[-1]:.6f}\")\n",
    "print(f\"Final Main Loss: {main_losses[-1]:.6f}\")\n",
    "print(f\"Final gradP (s1): {thermo1_losses[-1]:.6f}\")\n",
    "print(f\"Final gradT (s2): {thermo2_losses[-1]:.6f}\")\n",
    "print(f\"Final gradC (s3): {thermo3_losses[-1]:.6f}\")\n",
    "print(f\"\\nLoss data saved to: {csv_path}\")\n",
    "print(f\"Physics loss details saved to: {physics_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARITY PLOT GENERATION\n",
    "def collect_predictions_and_true_values(model, data_loader, device):\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            predictions.extend(output.cpu().numpy())\n",
    "            true_values.extend(data.aco2.cpu().numpy())\n",
    "    \n",
    "    return predictions, true_values\n",
    "\n",
    "# Function to calculate R² and RMSE\n",
    "def calculate_metrics(true_values, predictions):\n",
    "    r2 = r2_score(true_values, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predictions))\n",
    "    return r2, rmse\n",
    "\n",
    "# Function to save metrics to CSV\n",
    "def save_metrics_to_csv(r2_train, rmse_train, \n",
    "                        r2_val, rmse_val, \n",
    "                        r2_test, rmse_test, \n",
    "                        parent_directory):\n",
    "    # Create the metrics dictionary\n",
    "    metrics_data = {\n",
    "        'Dataset': ['Training', 'Validation', 'Test'],\n",
    "        'R2': [r2_train, r2_val, r2_test],\n",
    "        'RMSE': [rmse_train, rmse_val, rmse_test]\n",
    "    }\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(metrics_data)\n",
    "    \n",
    "    # Create filename with same pattern as model\n",
    "    filename = f\"metric_{seed}_{runtime}.csv\"\n",
    "    filepath = os.path.join(f\"{parent_directory}/models/models_root/{runtyp}/metric_csv\", filename)\n",
    "    os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "    # Save to CSV\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"Metrics saved to: {filepath}\")\n",
    "\n",
    "# Function to plot the parity plot\n",
    "def plot_parity_plot(train_true_values, train_predictions, \n",
    "                     val_true_values, val_predictions, \n",
    "                     test_true_values, test_predictions\n",
    "                     ):\n",
    "    fontsize = 17\n",
    "    matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "    # Calculate metrics\n",
    "    r2_train, rmse_train = calculate_metrics(train_true_values, train_predictions)\n",
    "    r2_val, rmse_val    = calculate_metrics(val_true_values, val_predictions)\n",
    "    r2_test, rmse_test = calculate_metrics(test_true_values, test_predictions)\n",
    "\n",
    "    # Unified parity plot\n",
    "    plt.figure(figsize=(7, 7))\n",
    "\n",
    "    # Scatter plots\n",
    "    plt.scatter(train_true_values, train_predictions, \n",
    "                edgecolors='b', alpha=0.6, c='b', marker='o', label=f'Train (R²={r2_train:.3f}, RMSE={rmse_train:.3f})')\n",
    "    plt.scatter(val_true_values, val_predictions, \n",
    "                edgecolors='g', alpha=0.6, c='g', marker='^', label=f'Val (R²={r2_val:.3f}, RMSE={rmse_val:.3f})')\n",
    "    plt.scatter(test_true_values, test_predictions, \n",
    "                edgecolors='r', alpha=0.6, c='r', marker='v', label=f'Test (R²={r2_test:.3f}, RMSE={rmse_test:.3f})')\n",
    "\n",
    "    # Parity line\n",
    "    max_val = max(max(train_true_values), max(val_true_values), max(test_true_values))\n",
    "    plt.plot([-0.1, max_val+0.5], [-0.1, max_val+0.5], 'k--', linewidth=2)\n",
    "\n",
    "    # Labels & ticks\n",
    "    plt.xlabel('Actual Solubility', fontsize=fontsize)\n",
    "    plt.ylabel('Predicted Solubility', fontsize=fontsize)\n",
    "    plt.xlim(-0.1, 2.5)\n",
    "    plt.ylim(-0.1, 2.5)\n",
    "    plt.xticks(fontsize=fontsize)\n",
    "    plt.yticks(fontsize=fontsize)\n",
    "    plt.minorticks_on()\n",
    "\n",
    "    # Customize tick appearance\n",
    "    plt.tick_params(axis='both', which='major', length=6, width=0.8)   # longer, thicker major ticks\n",
    "    plt.tick_params(axis='both', which='minor', length=4, width=0.8)   # shorter, thinner minor ticks\n",
    "\n",
    "    # Legend\n",
    "    plt.legend(fontsize=fontsize-3, loc='upper left', frameon=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Save metrics if needed\n",
    "    save_metrics_to_csv(r2_train, rmse_train, r2_val, rmse_val, r2_test, rmse_test, parent_directory)\n",
    "\n",
    "# Collect predictions and true values for training, validation, and test data\n",
    "train_predictions, train_true_values = collect_predictions_and_true_values(model, train_loader, device)\n",
    "val_predictions, val_true_values = collect_predictions_and_true_values(model, val_loader, device)\n",
    "test_predictions, test_true_values = collect_predictions_and_true_values(model, test_loader, device)\n",
    "\n",
    "# Plot the parity plot\n",
    "plot_parity_plot(train_true_values, train_predictions, \n",
    "                 val_true_values, val_predictions, \n",
    "                 test_true_values, test_predictions\n",
    "                 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
