{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING LIBRARIES\n",
    "import copy\n",
    "import os\n",
    "import random\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'  # or ':16:8'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "# FUNCTIONS\n",
    "from data_processing import load_dataset, process_dataset\n",
    "from path_helpers import get_path\n",
    "from stats_compute import compute_statistics, scale_graphs\n",
    "from smart_loader import load_model_for_inference\n",
    "from EnhancedDataSplit import DataSplitter\n",
    "from typing import List\n",
    "# DIRECTORY SETUP\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETER SETTINGS\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# Reproducibility settings\n",
    "seed = 21\n",
    "split_seed = 42\n",
    "batch_size = 32\n",
    "runtime = timestamp\n",
    "selected_device = 'cuda' # either 'cuda' or 'cpu\n",
    "device = torch.device(selected_device)\n",
    "\n",
    "# CUDA Deterministic (ON/OFF SETTING)\n",
    "# For PyTorch\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.use_deterministic_algorithms(False)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print('device           :', device)\n",
    "print('seed             :', seed)\n",
    "print('split seed       :', split_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD & GRAPH GENERATION FOR RAS\n",
    "df_components = load_dataset(get_path(file_name = 'components_set.csv', folder_name='datasets'))\n",
    "smiles_dict = dict(zip(df_components['Abbreviation'], df_components['SMILES']))\n",
    "df_systems = load_dataset(get_path(file_name = 'systems_set.csv', folder_name='datasets'))\n",
    "smiles_list = df_components[\"SMILES\"].dropna().tolist()\n",
    "mol_name_dict = smiles_dict.copy()\n",
    "# GRAPH\n",
    "system_graphs = process_dataset(df_systems, smiles_dict)\n",
    "# LOAD DATASET\n",
    "splitter = DataSplitter(system_graphs, random_state=split_seed)\n",
    "splitter.print_dataset_stats()\n",
    "# Options: rarity_aware_unseen_amine_split stratified_random_split\n",
    "train_data, val_data, test_data = splitter.rarity_aware_unseen_amine_split()\n",
    "#Retrieve the statistics of train_data\n",
    "stats = compute_statistics(train_data)\n",
    "conc_mean = stats[0]\n",
    "conc_std = stats[1]\n",
    "temp_mean = stats[2]\n",
    "temp_std = stats[3]\n",
    "pco2_mean = stats[4]\n",
    "pco2_std = stats[5]\n",
    "#Apply the scaling to validation and test\n",
    "original_train_data = copy.deepcopy(train_data)\n",
    "original_val_data = copy.deepcopy(val_data)\n",
    "original_test_data = copy.deepcopy(test_data)\n",
    "combined_original_data = original_train_data + original_val_data + original_test_data\n",
    "train_data = scale_graphs(train_data, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    "val_data = scale_graphs(val_data, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    "test_data = scale_graphs(test_data, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    "#Load the data into DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing varience of the alpha_co2 on datasets\n",
    "def get_alpha_stats(dataset):\n",
    "    values = torch.cat([torch.tensor([data.aco2]) for data in dataset])\n",
    "    \n",
    "    # Variance calculations\n",
    "    sample_var = torch.var(values, unbiased=True)      # N-1\n",
    "    population_var = torch.var(values, unbiased=False) # N\n",
    "    \n",
    "    # Range and statistics\n",
    "    min_val = torch.min(values)\n",
    "    max_val = torch.max(values)\n",
    "    mean_val = torch.mean(values)\n",
    "    std_val = torch.std(values, unbiased=True)\n",
    "    median_val = torch.median(values)\n",
    "    \n",
    "    return {\n",
    "        'sample_var': sample_var.item(),\n",
    "        'population_var': population_var.item(),\n",
    "        'min': min_val.item(),\n",
    "        'max': max_val.item(),\n",
    "        'range': (max_val - min_val).item(),\n",
    "        'mean': mean_val.item(),\n",
    "        'std': std_val.item(),\n",
    "        'median': median_val.item(),\n",
    "        'count': len(values)\n",
    "    }\n",
    "\n",
    "# Calculate statistics for all datasets\n",
    "train_stats = get_alpha_stats(original_train_data)\n",
    "val_stats = get_alpha_stats(original_val_data)\n",
    "test_stats = get_alpha_stats(original_test_data)\n",
    "\n",
    "# Print comprehensive statistics\n",
    "print(\"=\" * 80)\n",
    "print(\"α_CO2 DATASET STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "datasets = [('Train', train_stats), ('Val', val_stats), ('Test', test_stats)]\n",
    "\n",
    "for name, stats in datasets:\n",
    "    print(f\"\\n{name:>5} Dataset (n={stats['count']}):\")\n",
    "    print(f\"  Range:     [{stats['min']:.4f}, {stats['max']:.4f}]  (span: {stats['range']:.4f})\")\n",
    "    print(f\"  Mean:      {stats['mean']:.4f}  ±  {stats['std']:.4f}\")\n",
    "    print(f\"  Median:    {stats['median']:.4f}\")\n",
    "    print(f\"  Variance:  {stats['sample_var']:.6f} (sample), {stats['population_var']:.6f} (population)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RELATIVE COMPARISONS (vs Train)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for name, stats in datasets[1:]:  # Skip train for comparison\n",
    "    print(f\"\\n{name} vs Train:\")\n",
    "    print(f\"  Range ratio:    {stats['range']/train_stats['range']:.3f}\")\n",
    "    print(f\"  Mean ratio:     {stats['mean']/train_stats['mean']:.3f}\")\n",
    "    print(f\"  Variance ratio: {stats['sample_var']/train_stats['sample_var']:.3f}\")\n",
    "    print(f\"  Min overlap:    {'Yes' if stats['min'] >= train_stats['min'] else 'No'} ({stats['min']:.4f} vs {train_stats['min']:.4f})\")\n",
    "    print(f\"  Max overlap:    {'Yes' if stats['max'] <= train_stats['max'] else 'No'} ({stats['max']:.4f} vs {train_stats['max']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing model weights\n",
    "# Baseline\n",
    "baseline_dir = os.path.join(\n",
    "    os.path.dirname(os.getcwd()),\n",
    "    \"models\",\n",
    "    \"models_root\",\n",
    "    \"model_for_inference\",\n",
    "    \"ras_baseline\"\n",
    ")\n",
    "files = sorted(os.listdir(baseline_dir))\n",
    "if len(files) == 0:\n",
    "    raise FileNotFoundError(f\"No files found in {baseline_dir}\")\n",
    "model_file = files[0]\n",
    "print(f\"Using model file: {model_file}\")\n",
    "path = os.path.join(baseline_dir, model_file)\n",
    "selected_device = 'cuda'\n",
    "model_1 = load_model_for_inference(path, device=device)\n",
    "\n",
    "# PIGNN\n",
    "pinn_dir = os.path.join(\n",
    "    os.path.dirname(os.getcwd()),\n",
    "    \"models\",\n",
    "    \"models_root\",\n",
    "    \"model_for_inference\",\n",
    "    \"ras_pinn\"\n",
    ")\n",
    "files = sorted(os.listdir(pinn_dir))\n",
    "if len(files) == 0:\n",
    "    raise FileNotFoundError(f\"No files found in {pinn_dir}\")\n",
    "model_file = files[0]\n",
    "print(f\"Using model file: {model_file}\")\n",
    "path = os.path.join(pinn_dir, model_file)\n",
    "selected_device = 'cuda'\n",
    "model_2 = load_model_for_inference(path, device=device)\n",
    "\n",
    "# Load both models and prepare for comparison\n",
    "models = {\n",
    "    \"Baseline\": (model_1, lambda g: scale_graphs(g, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)),\n",
    "    \"PINN\": (model_2, lambda g: scale_graphs(g, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # PARITY PLOT GENERATION\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import torch\n",
    "import matplotlib\n",
    "\n",
    "def collect_predictions_and_true_values(model, data_loader, device):\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            predictions.extend(output.cpu().numpy())\n",
    "            true_values.extend(data.aco2.cpu().numpy())\n",
    "    \n",
    "    return np.array(predictions), np.array(true_values)\n",
    "\n",
    "# Function to calculate R² and RMSE\n",
    "def calculate_metrics(true_values, predictions):\n",
    "    r2 = r2_score(true_values, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predictions))\n",
    "    return r2, rmse\n",
    "\n",
    "# Function to save metrics to CSV\n",
    "def save_metrics_to_csv(metrics_dict, parent_directory):\n",
    "    import pandas as pd\n",
    "    # Create the metrics dictionary\n",
    "    metrics_data = {\n",
    "        'Dataset': [],\n",
    "        'R2': [],\n",
    "        'RMSE': []\n",
    "    }\n",
    "    \n",
    "    for dataset_name, metrics in metrics_dict.items():\n",
    "        metrics_data['Dataset'].append(dataset_name)\n",
    "        metrics_data['R2'].append(metrics['r2'])\n",
    "        metrics_data['RMSE'].append(metrics['rmse'])\n",
    "    \n",
    "    df = pd.DataFrame(metrics_data)\n",
    "    df.to_csv(f\"{parent_directory}/metrics.csv\", index=False)\n",
    "\n",
    "# Function to plot the parity plot with marginal histograms\n",
    "def plot_parity_plot(datasets_dict, parent_directory=None, fontsize=16):\n",
    "    \"\"\"\n",
    "    Plot parity plot for any combination of datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    datasets_dict : dict\n",
    "        Dictionary containing datasets to plot. Format:\n",
    "        {\n",
    "            'train': {'true': train_true_values, 'pred': train_predictions, 'color': 'b', 'marker': 'o'},\n",
    "            'val': {'true': val_true_values, 'pred': val_predictions, 'color': 'g', 'marker': '^'},\n",
    "            'test': {'true': test_true_values, 'pred': test_predictions, 'color': 'r', 'marker': 'v'}\n",
    "        }\n",
    "        You can include any subset of these keys.\n",
    "    parent_directory : str, optional\n",
    "        Directory to save metrics CSV\n",
    "    fontsize : int, optional\n",
    "        Font size for plot elements\n",
    "    \"\"\"\n",
    "    matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "    # Calculate metrics for each dataset\n",
    "    metrics_dict = {}\n",
    "    for dataset_name, data in datasets_dict.items():\n",
    "        r2, rmse = calculate_metrics(data['true'], data['pred'])\n",
    "        metrics_dict[dataset_name] = {'r2': r2, 'rmse': rmse}\n",
    "\n",
    "    # Create figure with gridspec for histograms\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    gs = gridspec.GridSpec(2, 2, width_ratios=[4, 1], height_ratios=[1, 4], \n",
    "                          hspace=0.00, wspace=0.00)\n",
    "    \n",
    "    # Main plot\n",
    "    ax = fig.add_subplot(gs[1, 0])\n",
    "    ax_histx = fig.add_subplot(gs[0, 0], sharex=ax)\n",
    "    ax_histy = fig.add_subplot(gs[1, 1], sharey=ax)\n",
    "\n",
    "    # Scatter plots for each dataset\n",
    "    for dataset_name, data in datasets_dict.items():\n",
    "        label = f\"{dataset_name.capitalize()} (R² = {metrics_dict[dataset_name]['r2']:.4f}, RMSE = {metrics_dict[dataset_name]['rmse']:.4f})\"\n",
    "        ax.scatter(data['true'], data['pred'], \n",
    "                   edgecolors=data['color'], alpha=0.5, c=data['color'], \n",
    "                   marker=data['marker'], label=label)\n",
    "\n",
    "    # Parity line\n",
    "    all_true_values = np.concatenate([data['true'] for data in datasets_dict.values()])\n",
    "    max_val = max(all_true_values) if len(all_true_values) > 0 else 2.5\n",
    "    ax.plot([-0.1, 10+0.5], [-0.1, 10+0.5], '--', linewidth=1.5, color='black')\n",
    "\n",
    "    # Labels & ticks\n",
    "    ax.set_xlabel('Actual Solubility', fontsize=fontsize)\n",
    "    ax.set_ylabel('Predicted Solubility', fontsize=fontsize)\n",
    "    ax.set_xlim(-0.1, 2.5)\n",
    "    ax.set_ylim(-0.1, 2.5)\n",
    "    ax.tick_params(axis='both', which='major', length=6, width=0.8, labelsize=fontsize)\n",
    "    ax.tick_params(axis='both', which='minor', length=4, width=0.8)\n",
    "    ax.minorticks_on()\n",
    "    ax.legend(fontsize=fontsize-3, loc='upper left', frameon=False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)    \n",
    "\n",
    "    # Add histograms with dataset differentiation\n",
    "    bins = np.linspace(-0.1, 2.5, 27)\n",
    "    \n",
    "    # Top histogram (experimental values)\n",
    "    if datasets_dict:\n",
    "        # Convert to numpy arrays and flatten\n",
    "        true_data = [data['true'].flatten() for data in datasets_dict.values()]\n",
    "        colors = [data['color'] for data in datasets_dict.values()]\n",
    "        \n",
    "        ax_histx.hist(true_data, bins=bins, color=colors, \n",
    "                      alpha=0.5, stacked=True, edgecolor='black', linewidth=0.5)\n",
    "        ax_histx.tick_params(labelbottom=False, labelleft=False, left=False)\n",
    "        ax_histx.spines['top'].set_visible(False)\n",
    "        ax_histx.spines['right'].set_visible(False)\n",
    "        ax_histx.spines['left'].set_visible(False)\n",
    "\n",
    "    # Right histogram (predicted values)\n",
    "    if datasets_dict:\n",
    "        # Convert to numpy arrays and flatten\n",
    "        pred_data = [data['pred'].flatten() for data in datasets_dict.values()]\n",
    "        colors = [data['color'] for data in datasets_dict.values()]\n",
    "        \n",
    "        ax_histy.hist(pred_data, bins=bins, orientation='horizontal', \n",
    "                      color=colors, alpha=0.5, stacked=True, \n",
    "                      edgecolor='black', linewidth=0.5)\n",
    "        ax_histy.tick_params(labelbottom=False, labelleft=False, bottom=False)\n",
    "        ax_histy.spines['top'].set_visible(False)\n",
    "        ax_histy.spines['right'].set_visible(False)\n",
    "        ax_histy.spines['bottom'].set_visible(False)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Save metrics if needed\n",
    "    if parent_directory:\n",
    "        save_metrics_to_csv(metrics_dict, parent_directory)\n",
    "\n",
    "\n",
    "# Collect predictions and true values for training, validation, and test data\n",
    "train_predictions, train_true_values = collect_predictions_and_true_values(model_2, train_loader, device)\n",
    "val_predictions, val_true_values = collect_predictions_and_true_values(model_2, val_loader, device)\n",
    "test_predictions, test_true_values = collect_predictions_and_true_values(model_2, test_loader, device)\n",
    "\n",
    "# EXAMPLE USAGE - Choose any combination:\n",
    "\n",
    "# Option 1: All three datasets\n",
    "plot_parity_plot({\n",
    "    'train': {'true': train_true_values, 'pred': train_predictions, 'color': 'b', 'marker': 'o'},\n",
    "    'val': {'true': val_true_values, 'pred': val_predictions, 'color': 'g', 'marker': '^'},\n",
    "    'test': {'true': test_true_values, 'pred': test_predictions, 'color': 'r', 'marker': 'v'}\n",
    "})\n",
    "\n",
    "# Option 2: Only train and validation\n",
    "plot_parity_plot({\n",
    "    'train': {'true': train_true_values, 'pred': train_predictions, 'color': 'b', 'marker': 'o'},\n",
    "    'val': {'true': val_true_values, 'pred': val_predictions, 'color': 'g', 'marker': '^'}\n",
    "})\n",
    "\n",
    "# Option 3: Only train and test\n",
    "plot_parity_plot({\n",
    "    'train': {'true': train_true_values, 'pred': train_predictions, 'color': 'b', 'marker': 'o'},\n",
    "    'test': {'true': test_true_values, 'pred': test_predictions, 'color': 'r', 'marker': 'v'}\n",
    "})\n",
    "\n",
    "# Option 4: Only validation and test\n",
    "plot_parity_plot({\n",
    "    'val': {'true': val_true_values, 'pred': val_predictions, 'color': 'g', 'marker': '^'},\n",
    "    'test': {'true': test_true_values, 'pred': test_predictions, 'color': 'r', 'marker': 'v'}\n",
    "})\n",
    "\n",
    "# Option 5: Only train\n",
    "plot_parity_plot({\n",
    "    'train': {'true': train_true_values, 'pred': train_predictions, 'color': 'b', 'marker': 'o'}\n",
    "})\n",
    "\n",
    "# Option 6: Only validation\n",
    "plot_parity_plot({\n",
    "    'val': {'true': val_true_values, 'pred': val_predictions, 'color': 'g', 'marker': '^'}\n",
    "})\n",
    "\n",
    "# Option 7: Only test\n",
    "plot_parity_plot({\n",
    "    'test': {'true': test_true_values, 'pred': test_predictions, 'color': 'r', 'marker': 'v'}\n",
    "}) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # PARITY PLOT GENERATION (isolated)\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import torch\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "\n",
    "def collect_predictions_and_true_values_with_amines(model, data_loader, device):\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "    amine_names = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            predictions.extend(output.cpu().numpy())\n",
    "            true_values.extend(data.aco2.cpu().numpy())\n",
    "            \n",
    "            # Extract amine names - handle different possible formats\n",
    "            if hasattr(data, 'name'):\n",
    "                # If name is a tensor, convert to numpy and then to list\n",
    "                if torch.is_tensor(data.name):\n",
    "                    amine_names.extend(data.name.cpu().numpy().tolist())\n",
    "                # If name is already a list or array\n",
    "                elif isinstance(data.name, (list, np.ndarray)):\n",
    "                    amine_names.extend(data.name)\n",
    "                else:\n",
    "                    # Fallback if format is unexpected\n",
    "                    amine_names.extend(['unknown'] * len(data))\n",
    "            else:\n",
    "                amine_names.extend(['unknown'] * len(data))\n",
    "    \n",
    "    return np.array(predictions), np.array(true_values), amine_names\n",
    "\n",
    "# Function to calculate R² and RMSE\n",
    "def calculate_metrics(true_values, predictions):\n",
    "    r2 = r2_score(true_values, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predictions))\n",
    "    return r2, rmse\n",
    "\n",
    "# Function to plot the parity plot with amine-based coloring\n",
    "def plot_test_parity_plot_by_amine(test_true_values, test_predictions, amine_names, fontsize=16):\n",
    "    \"\"\"\n",
    "    Plot parity plot for test set with points colored by amine names.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    test_true_values : array-like\n",
    "        True values for test set\n",
    "    test_predictions : array-like\n",
    "        Predicted values for test set\n",
    "    amine_names : list\n",
    "        List of amine names corresponding to each data point\n",
    "    fontsize : int, optional\n",
    "        Font size for plot elements\n",
    "    \"\"\"\n",
    "    matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "    # Calculate metrics\n",
    "    r2_test, rmse_test = calculate_metrics(test_true_values, test_predictions)\n",
    "\n",
    "    # Create figure with gridspec for histograms\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    gs = gridspec.GridSpec(2, 2, width_ratios=[4, 1], height_ratios=[1, 4], \n",
    "                          hspace=0.00, wspace=0.00)\n",
    "    \n",
    "    # Main plot\n",
    "    ax = fig.add_subplot(gs[1, 0])\n",
    "    ax_histx = fig.add_subplot(gs[0, 0], sharex=ax)\n",
    "    ax_histy = fig.add_subplot(gs[1, 1], sharey=ax)\n",
    "\n",
    "    # Get unique amine names and assign colors\n",
    "    unique_amines = sorted(set(amine_names))\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, len(unique_amines)))\n",
    "    \n",
    "    # Create color mapping\n",
    "    amine_color_map = {amine: colors[i] for i, amine in enumerate(unique_amines)}\n",
    "    \n",
    "    # Convert amine_names to numpy array for boolean indexing\n",
    "    amine_names_array = np.array(amine_names)\n",
    "    \n",
    "    # Scatter plot colored by amine\n",
    "    for amine in unique_amines:\n",
    "        mask = amine_names_array == amine\n",
    "        if np.any(mask):  # Only plot if there are points for this amine\n",
    "            ax.scatter(test_true_values[mask], test_predictions[mask], \n",
    "                       color=amine_color_map[amine], alpha=0.7, s=50,\n",
    "                       label=amine, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "    # Parity line\n",
    "    ax.plot([-0.1, 10], [-0.1, 10], '--', linewidth=1.5, color='black')\n",
    "\n",
    "    # Labels & ticks\n",
    "    ax.set_xlabel('Actual Solubility', fontsize=fontsize)\n",
    "    ax.set_ylabel('Predicted Solubility', fontsize=fontsize)\n",
    "    ax.set_xlim(-0.1, 2.5)\n",
    "    ax.set_ylim(-0.1, 2.5)\n",
    "    ax.tick_params(axis='both', which='major', length=6, width=0.8, labelsize=fontsize)\n",
    "    ax.tick_params(axis='both', which='minor', length=4, width=0.8)\n",
    "    ax.minorticks_on()\n",
    "    ax.legend(fontsize=fontsize-4, loc='upper left', frameon=True, ncol=2)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    # Add title with metrics\n",
    "    ax.set_title(f'Test Set: R² = {r2_test:.4f}, RMSE = {rmse_test:.4f}', fontsize=fontsize)\n",
    "\n",
    "    # Add histograms with amine-based coloring\n",
    "    bins = np.linspace(-0.1, 2.5, 27)\n",
    "    \n",
    "    # Top histogram (experimental values) - colored by amine\n",
    "    if len(test_true_values) > 0:\n",
    "        # Filter out empty arrays\n",
    "        true_data = [test_true_values[amine_names_array == amine] for amine in unique_amines]\n",
    "        true_data = [arr for arr in true_data if len(arr) > 0]\n",
    "        true_colors = [amine_color_map[amine] for amine in unique_amines if len(test_true_values[amine_names_array == amine]) > 0]\n",
    "        \n",
    "        if true_data:  # Only plot if there's data\n",
    "            ax_histx.hist(true_data, bins=bins, color=true_colors, \n",
    "                          alpha=0.7, stacked=True, edgecolor='black', linewidth=0.5)\n",
    "        ax_histx.tick_params(labelbottom=False, labelleft=False, left=False)\n",
    "        ax_histx.spines['top'].set_visible(False)\n",
    "        ax_histx.spines['right'].set_visible(False)\n",
    "        ax_histx.spines['left'].set_visible(False)\n",
    "\n",
    "    # Right histogram (predicted values) - colored by amine\n",
    "    if len(test_predictions) > 0:\n",
    "        # Filter out empty arrays and ensure they are 1D\n",
    "        pred_data = [test_predictions[amine_names_array == amine] for amine in unique_amines]\n",
    "        pred_data = [arr.flatten() for arr in pred_data if len(arr) > 0]  # Flatten to ensure 1D\n",
    "        pred_colors = [amine_color_map[amine] for amine in unique_amines if len(test_predictions[amine_names_array == amine]) > 0]\n",
    "        \n",
    "        if pred_data:  # Only plot if there's data\n",
    "            ax_histy.hist(pred_data, bins=bins, orientation='horizontal', \n",
    "                          color=pred_colors, alpha=0.7, stacked=True, \n",
    "                          edgecolor='black', linewidth=0.5)\n",
    "        ax_histy.tick_params(labelbottom=False, labelleft=False, bottom=False)\n",
    "        ax_histy.spines['top'].set_visible(False)\n",
    "        ax_histy.spines['right'].set_visible(False)\n",
    "        ax_histy.spines['bottom'].set_visible(False)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Test Set Metrics:\")\n",
    "    print(f\"R²: {r2_test:.4f}\")\n",
    "    print(f\"RMSE: {rmse_test:.4f}\")\n",
    "    print(f\"Number of amines: {len(unique_amines)}\")\n",
    "    print(f\"Amines: {', '.join(unique_amines)}\")\n",
    "    \n",
    "    # Print count per amine\n",
    "    for amine in unique_amines:\n",
    "        count = np.sum(amine_names_array == amine)\n",
    "        print(f\"{amine}: {count} points\")\n",
    "\n",
    "# Collect predictions, true values, and amine names for test data\n",
    "test_predictions, test_true_values, test_amine_names = collect_predictions_and_true_values_with_amines(model_2, test_loader, device)\n",
    "\n",
    "# Debug: Check what we got\n",
    "print(f\"Number of predictions: {len(test_predictions)}\")\n",
    "print(f\"Number of true values: {len(test_true_values)}\")\n",
    "print(f\"Number of amine names: {len(test_amine_names)}\")\n",
    "print(f\"First few amine names: {test_amine_names[:10]}\")\n",
    "print(f\"Unique amine names: {sorted(set(test_amine_names))}\")\n",
    "\n",
    "# Check if arrays are 1D\n",
    "print(f\"Predictions shape: {test_predictions.shape}\")\n",
    "print(f\"True values shape: {test_true_values.shape}\")\n",
    "\n",
    "# If predictions/true values are 2D, flatten them\n",
    "if len(test_predictions.shape) > 1:\n",
    "    test_predictions = test_predictions.flatten()\n",
    "    print(f\"Flattened predictions shape: {test_predictions.shape}\")\n",
    "\n",
    "if len(test_true_values.shape) > 1:\n",
    "    test_true_values = test_true_values.flatten()\n",
    "    print(f\"Flattened true values shape: {test_true_values.shape}\")\n",
    "\n",
    "# Plot the test set parity plot colored by amine\n",
    "plot_test_parity_plot_by_amine(test_true_values, test_predictions, test_amine_names) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AAD CALCULATION\n",
    "models = {\n",
    "    \"Baseline\": (model_1, lambda g: scale_graphs(g, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)),\n",
    "    \"PINN\": (model_2, lambda g: scale_graphs(g, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)),\n",
    "}\n",
    "\n",
    "# Min points per temperature (filtering)\n",
    "min_points_per_temp = 2\n",
    "\n",
    "used_data_for_inference = original_test_data\n",
    "# Extract unique values from the data\n",
    "unique_systems = list(set(graph.name for graph in used_data_for_inference))\n",
    "unique_concentrations = list(set(graph.conc for graph in used_data_for_inference))\n",
    "unique_temperatures = list(set(graph.temp for graph in used_data_for_inference))\n",
    "unique_references = list(set(graph.ref for graph in used_data_for_inference))\n",
    "# Store AAD results\n",
    "aad_results = {model_name: {} for model_name in models.keys()}\n",
    "\n",
    "for amine in unique_systems:\n",
    "    # Collect all graphs for this amine across all refs and concentrations\n",
    "    graphs_amine = [g for g in used_data_for_inference if g.name == amine]\n",
    "    \n",
    "    # Temperature filtering\n",
    "    temp_counts = {temp: sum(1 for g in graphs_amine if g.temp == temp) for temp in set(g.temp for g in graphs_amine)}\n",
    "    filtered_temps = sorted([t for t, count in temp_counts.items() if count >= min_points_per_temp])\n",
    "    \n",
    "    if not filtered_temps:\n",
    "        continue  # skip amines without enough points\n",
    "    \n",
    "    # Keep only graphs in filtered temperatures\n",
    "    graphs_filtered = [g for g in graphs_amine if g.temp in filtered_temps]\n",
    "    \n",
    "    # For each model\n",
    "    for model_name, (model, scaler) in models.items():\n",
    "        abs_errors = []\n",
    "        for g in graphs_filtered:\n",
    "            g_pred = g.clone()\n",
    "            \n",
    "            # Ensure all scalar features are tensors\n",
    "            g_pred.temp = torch.tensor([g_pred.temp], dtype=torch.float)\n",
    "            g_pred.conc = torch.tensor([g_pred.conc], dtype=torch.float)\n",
    "            g_pred.pco2 = torch.tensor([g_pred.pco2], dtype=torch.float)\n",
    "            \n",
    "            g_pred = scaler(g_pred).to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(g_pred).cpu().numpy().flatten()\n",
    "            \n",
    "            abs_errors.append(np.abs(pred - g_pred.aco2))  # g_pred.aco2 can stay float\n",
    "            \n",
    "        # Average absolute deviation\n",
    "        aad_results[model_name][amine] = np.mean(abs_errors)\n",
    "\n",
    "# Print AAD per amine per model\n",
    "for model_name, amines in aad_results.items():\n",
    "    print(f\"\\n=== {model_name} AAD per amine ===\")\n",
    "    for amine, aad in amines.items():\n",
    "        print(f\"{amine}: {aad:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE CALCULATION\n",
    "models = {\n",
    "    \"Baseline\": (model_1, lambda g: scale_graphs(g, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)),\n",
    "    \"PINN\": (model_2, lambda g: scale_graphs(g, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)),\n",
    "}\n",
    "\n",
    "# Min points per temperature (filtering)\n",
    "min_points_per_temp = 2\n",
    "used_data_for_inference = original_test_data\n",
    "# Extract unique values from the data\n",
    "unique_systems = list(set(graph.name for graph in used_data_for_inference))\n",
    "unique_concentrations = list(set(graph.conc for graph in used_data_for_inference))\n",
    "unique_temperatures = list(set(graph.temp for graph in used_data_for_inference))\n",
    "unique_references = list(set(graph.ref for graph in used_data_for_inference))\n",
    "# Store RMSE results\n",
    "rmse_results = {model_name: {} for model_name in models.keys()}\n",
    "\n",
    "for amine in unique_systems:\n",
    "    # Collect all graphs for this amine across all refs and concentrations\n",
    "    graphs_amine = [g for g in used_data_for_inference if g.name == amine]\n",
    "    \n",
    "    # Temperature filtering\n",
    "    temp_counts = {temp: sum(1 for g in graphs_amine if g.temp == temp) for temp in set(g.temp for g in graphs_amine)}\n",
    "    filtered_temps = sorted([t for t, count in temp_counts.items() if count >= min_points_per_temp])\n",
    "    \n",
    "    if not filtered_temps:\n",
    "        continue  # skip amines without enough points\n",
    "    \n",
    "    # Keep only graphs in filtered temperatures\n",
    "    graphs_filtered = [g for g in graphs_amine if g.temp in filtered_temps]\n",
    "    \n",
    "    # For each model\n",
    "    for model_name, (model, scaler) in models.items():\n",
    "        squared_errors = []\n",
    "        for g in graphs_filtered:\n",
    "            g_pred = g.clone()\n",
    "            \n",
    "            # Ensure all scalar features are tensors\n",
    "            g_pred.temp = torch.tensor([g_pred.temp], dtype=torch.float)\n",
    "            g_pred.conc = torch.tensor([g_pred.conc], dtype=torch.float)\n",
    "            g_pred.pco2 = torch.tensor([g_pred.pco2], dtype=torch.float)\n",
    "            \n",
    "            g_pred = scaler(g_pred).to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(g_pred).cpu().numpy().flatten()\n",
    "            \n",
    "            # Calculate squared error\n",
    "            squared_errors.append((pred - g_pred.aco2) ** 2)\n",
    "        \n",
    "        # Root Mean Square Error\n",
    "        rmse_results[model_name][amine] = np.sqrt(np.mean(squared_errors))\n",
    "\n",
    "# Print RMSE per amine per model\n",
    "for model_name, amines in rmse_results.items():\n",
    "    print(f\"\\n=== {model_name} RMSE per amine ===\")\n",
    "    for amine, rmse in amines.items():\n",
    "        print(f\"{amine}: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAPE CALCULATION\n",
    "models = {\n",
    "    \"Baseline\": (model_1, lambda g: scale_graphs(g, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)),\n",
    "    \"PINN\": (model_2, lambda g: scale_graphs(g, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)),\n",
    "}\n",
    "\n",
    "# Min points per temperature (filtering)\n",
    "min_points_per_temp = 2\n",
    "used_data_for_inference = original_test_data\n",
    "# Extract unique values from the data\n",
    "unique_systems = list(set(graph.name for graph in used_data_for_inference))\n",
    "unique_concentrations = list(set(graph.conc for graph in used_data_for_inference))\n",
    "unique_temperatures = list(set(graph.temp for graph in used_data_for_inference))\n",
    "unique_references = list(set(graph.ref for graph in used_data_for_inference))\n",
    "# Store MAPE results\n",
    "mape_results = {model_name: {} for model_name in models.keys()}\n",
    "\n",
    "for amine in unique_systems:\n",
    "    # Collect all graphs for this amine across all refs and concentrations\n",
    "    graphs_amine = [g for g in used_data_for_inference if g.name == amine]\n",
    "    \n",
    "    # Temperature filtering\n",
    "    temp_counts = {temp: sum(1 for g in graphs_amine if g.temp == temp) for temp in set(g.temp for g in graphs_amine)}\n",
    "    filtered_temps = sorted([t for t, count in temp_counts.items() if count >= min_points_per_temp])\n",
    "    \n",
    "    if not filtered_temps:\n",
    "        continue  # skip amines without enough points\n",
    "    \n",
    "    # Keep only graphs in filtered temperatures\n",
    "    graphs_filtered = [g for g in graphs_amine if g.temp in filtered_temps]\n",
    "    \n",
    "    # For each model\n",
    "    for model_name, (model, scaler) in models.items():\n",
    "        percentage_errors = []\n",
    "        for g in graphs_filtered:\n",
    "            g_pred = g.clone()\n",
    "            \n",
    "            # Ensure all scalar features are tensors\n",
    "            g_pred.temp = torch.tensor([g_pred.temp], dtype=torch.float)\n",
    "            g_pred.conc = torch.tensor([g_pred.conc], dtype=torch.float)\n",
    "            g_pred.pco2 = torch.tensor([g_pred.pco2], dtype=torch.float)\n",
    "            \n",
    "            g_pred = scaler(g_pred).to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(g_pred).cpu().numpy().flatten()\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            if g_pred.aco2 != 0:\n",
    "                percentage_errors.append(np.abs(pred - g_pred.aco2) / np.abs(g_pred.aco2))\n",
    "        \n",
    "        # Mean Absolute Percentage Error\n",
    "        mape_results[model_name][amine] = np.mean(percentage_errors) * 100\n",
    "\n",
    "# Print MAPE per amine per model\n",
    "for model_name, amines in mape_results.items():\n",
    "    print(f\"\\n=== {model_name} MAPE per amine (%) ===\")\n",
    "    for amine, mape in amines.items():\n",
    "        print(f\"{amine}: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize original_test_data to see available combinations for isotherm visualization\n",
    "# Get unique graph\n",
    "unique_named_graphs = {}\n",
    "for graph in test_data:\n",
    "    name = graph['name']\n",
    "    if name not in unique_named_graphs:\n",
    "        unique_named_graphs[name] = graph  # Keep the first occurrence\n",
    "\n",
    "# Optional: convert to a list\n",
    "unique_graph_list = list(unique_named_graphs.values())\n",
    "unique_list = DataLoader(unique_graph_list, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "data_summary = []\n",
    "for point in original_test_data:\n",
    "    data_summary.append({\n",
    "        'name': point['name'],\n",
    "        'temp': point.temp,\n",
    "        'conc': point.conc,\n",
    "        'pco2': point.pco2,\n",
    "        'aco2': point.aco2\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "df_summary = pd.DataFrame(data_summary)\n",
    "\n",
    "# Group by amine to see available conditions\n",
    "print(\"Available experimental conditions by amine:\")\n",
    "amines = {}  # Final object to store all amine data\n",
    "\n",
    "for amine in df_summary['name'].unique():\n",
    "    amine_data = df_summary[df_summary['name'] == amine]\n",
    "    print(f\"\\n{amine}:\")\n",
    "    \n",
    "    # List unique temperatures\n",
    "    unique_temps = sorted(amine_data['temp'].unique())\n",
    "    print(f\"  Temperatures (K): {[float(t) for t in unique_temps]}\")\n",
    "    \n",
    "    # List unique concentrations\n",
    "    unique_concs = sorted(amine_data['conc'].unique())\n",
    "    print(f\"  Concentrations (M): {[float(c) for c in unique_concs]}\")\n",
    "    \n",
    "    print(f\"  pCO2 range: {amine_data['pco2'].min():.2e} kPa - {amine_data['pco2'].max():.2e} kPa\")\n",
    "    print(f\"  Total data points: {len(amine_data)}\")\n",
    "    \n",
    "    # Store in final amines object\n",
    "    amines[amine] = {\n",
    "        'data': amine_data,  # Full DataFrame for this amine\n",
    "        'temperatures': [float(t) for t in unique_temps],\n",
    "        'concentrations': [float(c) for c in unique_concs],\n",
    "        'pco2_range': (float(amine_data['pco2'].min()), float(amine_data['pco2'].max())),\n",
    "        'num_points': len(amine_data),\n",
    "        'temp_conc_combinations': list(amine_data[['temp', 'conc']].drop_duplicates().itertuples(index=False, name=None))\n",
    "    }\n",
    "\n",
    "# Create a simple list of amine names\n",
    "amine_names = list(amines.keys())\n",
    "\n",
    "print(f\"\\n\\nFinal amines object created with {len(amines)} amines.\")\n",
    "print(\"Access data using: amines['amine_name']['property']\")\n",
    "print(\"Available properties: 'data', 'temperatures', 'concentrations', 'pco2_range', 'num_points', 'temp_conc_combinations'\")\n",
    "\n",
    "print(f\"\\n\\nAvailable amines ({len(amine_names)}):\")\n",
    "for i, name in enumerate(amine_names, 1):\n",
    "    print(f\"  {i}. {name}\")\n",
    "    \n",
    "print(f\"\\nUse amine_names list to access: {amine_names}\")\n",
    "print(\"Example: amines[amine_names[0]] to get data for the first amine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" fontsize = 16\n",
    "# Set Times New Roman font for all text\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "# Prediction setup\n",
    "pco2_values = np.logspace(0, 3, 100)\n",
    "system = 'DGA'\n",
    "temperature_values = [313.15, 353.15, 393.15, 0]\n",
    "constant_conc = 1\n",
    "\n",
    "# Define colors for each temperature\n",
    "# Colors: automatic assignment from colormap\n",
    "cmap = plt.get_cmap(\"Set1\")  # supports many distinct colors\n",
    "colors = {T: cmap(i) for i, T in enumerate(temperature_values)}\n",
    "# Define markers and linestyles\n",
    "linestyles = ['-', '--', '-.', ':']\n",
    "markers = ['o', 's', 'D', '^', 'v']\n",
    "\n",
    "\n",
    "# Filter the data for the specific system\n",
    "system_data = [graph for graph in original_test_data if graph.name == system and graph.aco2]\n",
    "plt.figure(figsize=(6, 5))\n",
    "\n",
    "# Iterate over each temperature and generate predictions for continuous lines\n",
    "for i, temp in enumerate(temperature_values):\n",
    "    predictions = []\n",
    "    \n",
    "    for pco2 in pco2_values:\n",
    "        graph = system_data[0].clone()\n",
    "        graph.temp = torch.tensor([temp], dtype=torch.float)\n",
    "        graph.conc = torch.tensor([constant_conc], dtype=torch.float)\n",
    "        graph.pco2 = torch.tensor([pco2], dtype=torch.float)\n",
    "        graph = scale_graphs(graph, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    "        graph = graph.to(device)\n",
    "        with torch.no_grad():\n",
    "            prediction = model_2(graph)\n",
    "            predictions.append(prediction.cpu().numpy().flatten())\n",
    "    \n",
    "    predictions = np.array(predictions).flatten()\n",
    "    \n",
    "    # Plot continuous prediction line with specified color and linestyle\n",
    "    plt.plot(pco2_values, predictions, color=colors[temp], linestyle=linestyles[i % len(linestyles)], label=f'Prediction at {temp} K')\n",
    "\n",
    "# Retrieving actual data (if exists) for the specific system\n",
    "filtered_graphs = [graph for graph in original_test_data \n",
    "                   if graph.name == system and\n",
    "                   graph.conc == constant_conc]\n",
    "\n",
    "# Store R² and RMSE values\n",
    "r2_values = {}\n",
    "rmse_values = {}\n",
    "\n",
    "# Collect actual data points and calculate errors for each temperature\n",
    "for i, temp in enumerate(temperature_values):\n",
    "    pco2_act = []\n",
    "    aco2_act = []\n",
    "    aco2_pred = []\n",
    "    \n",
    "    # Collect experimental points for this temperature\n",
    "    for graph in filtered_graphs:\n",
    "        if graph.temp == temp:\n",
    "            pco2_act.append(graph.pco2)\n",
    "            aco2_act.append(graph.aco2)\n",
    "    \n",
    "    # Generate predictions ONLY at experimental PCO2 values\n",
    "    for pco2 in pco2_act:\n",
    "        graph = system_data[0].clone()\n",
    "        graph.temp = torch.tensor([temp], dtype=torch.float)\n",
    "        graph.conc = torch.tensor([constant_conc], dtype=torch.float)\n",
    "        graph.pco2 = torch.tensor([pco2], dtype=torch.float)\n",
    "        graph = scale_graphs(graph, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    "        graph = graph.to(device)\n",
    "        with torch.no_grad():\n",
    "            prediction = model_2(graph)\n",
    "            aco2_pred.append(prediction.cpu().numpy().flatten()[0])\n",
    "\n",
    "    # Compute R² and RMSE\n",
    "    if len(aco2_act) > 0 and len(aco2_pred) > 0:\n",
    "        r2_values[temp] = r2_score(aco2_act, aco2_pred)\n",
    "        rmse_values[temp] = np.sqrt(mean_squared_error(aco2_act, aco2_pred))\n",
    "        \n",
    "        # Plot experimental points without error bars\n",
    "        plt.scatter(pco2_act, aco2_act, \n",
    "                   marker=markers[i % len(markers)],  # Different marker for each temp\n",
    "                   color=colors[temp],\n",
    "                   label=f'Experimental at {temp} K',\n",
    "                   s=50,  # marker size\n",
    "                   edgecolors='black',  # add black edge for better visibility\n",
    "                   linewidth=0.5)\n",
    "\n",
    "# Generate metric summary string\n",
    "metrics_text = \"\\n\".join([f\"T={temp} K: R²={r2_values[temp]:.2f}, RMSE={rmse_values[temp]:.2f}\" for temp in r2_values])\n",
    "\n",
    "# Plot customization\n",
    "plt.xlabel('CO$_{2}$ Partial Pressure (kPa)', fontsize=fontsize)\n",
    "plt.ylabel(r'CO$_{\\mathrm{2}}$ Loading (mol$_{\\mathrm{CO2}}$/mol$_{\\mathrm{A}}$)', fontsize=fontsize)\n",
    "\n",
    "# Add system name inside the plot area (top-left corner)\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "plt.text(0.05, 0.95, f'{system} at {constant_conc} M', \n",
    "         transform=plt.gca().transAxes, \n",
    "         fontsize=fontsize-2,\n",
    "         verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "plt.xticks(fontsize=fontsize)\n",
    "plt.yticks(fontsize=fontsize)\n",
    "plt.xlim(min(pco2_values), max(pco2_values))\n",
    "plt.ylim(0, 1.5)\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.gca().yaxis.set_major_locator(MultipleLocator(0.5))\n",
    "plt.gca().yaxis.set_minor_locator(MultipleLocator(0.25))\n",
    "plt.minorticks_on()\n",
    "\n",
    "plt.grid(True, which='both', linewidth=0.5)\n",
    "plt.legend(\n",
    "    bbox_to_anchor=(0, 1.02, 1, 0.2),\n",
    "    loc='lower left',\n",
    "    mode='expand',\n",
    "    frameon=False,\n",
    "    fontsize=fontsize-4,\n",
    "    ncol=2,\n",
    "    markerscale=1,\n",
    "    labelspacing=0.4,\n",
    "    borderpad=0.5,\n",
    "    columnspacing=1.0,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show() \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
