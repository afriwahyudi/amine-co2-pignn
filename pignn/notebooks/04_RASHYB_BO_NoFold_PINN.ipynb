{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WORKSPACE SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING LIBRARIES\n",
    "import copy\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "import joblib\n",
    "import logging\n",
    "from typing import Dict, Any, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Configure logging for Optuna\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdmolops\n",
    "from rdkit.Chem import rdDistGeom as molDG\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem.rdchem import GetPeriodicTable\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool, global_max_pool, global_add_pool\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn.inits import reset\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# FUNCTIONS\n",
    "from data_processing import load_dataset, smiles_to_graph, process_dataset, generate_graphs\n",
    "from path_helpers import get_path\n",
    "from stats_compute import compute_statistics, scale_graphs\n",
    "import ModelArchitecture\n",
    "from EnhancedDataSplit import DataSplitter\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import Tuple, List\n",
    "\n",
    "# DIRECTORY SETUP\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETER SETTINGS\n",
    "# Reproducibility settings\n",
    "use_physics_in_loss=True \n",
    "monitor_physics=True\n",
    "seed = 21\n",
    "split_seed = 42\n",
    "num_epochs = 100\n",
    "patience = 30\n",
    "runtyp = 'RASHYB_BO_PINN'\n",
    "study_name = '04_mean'\n",
    "print('Base seed        :', seed)\n",
    "print('Split seed       :', split_seed)\n",
    "print('Max epochs       :', num_epochs)\n",
    "print('Patience         :', patience)\n",
    "run_time = time.time()\n",
    "\n",
    "# CUDA Deterministic (ON/OFF SETTING)\n",
    "# For PyTorch\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.use_deterministic_algorithms(False)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "selected_device = 'cuda' # either 'cuda' or 'cpu\n",
    "device = torch.device(selected_device)\n",
    "print('device           :', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BAYESIAN OPTIMIZER - PHYSICS-INFORMED VERSION\n",
    "class BayesianOptimizer:\n",
    "    def __init__(self, \n",
    "                 train_data, \n",
    "                 val_data, \n",
    "                 test_data,\n",
    "                 conc_std,\n",
    "                 conc_mean,\n",
    "                 temp_std, \n",
    "                 temp_mean,\n",
    "                 pco2_std,\n",
    "                 pco2_mean,\n",
    "                 device,\n",
    "                 parent_directory,\n",
    "                 n_trials=10,\n",
    "                 seed=21):\n",
    "\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "        self.conc_std = conc_std\n",
    "        self.conc_mean = conc_mean\n",
    "        self.temp_std = temp_std\n",
    "        self.temp_mean = temp_mean\n",
    "        self.pco2_std = pco2_std\n",
    "        self.pco2_mean = pco2_mean\n",
    "        self.device = device\n",
    "        self.parent_directory = parent_directory\n",
    "        self.n_trials = n_trials\n",
    "        self.seed = seed\n",
    "        \n",
    "        # Get dimensions from data\n",
    "        self.node_dim = train_data[0].x.size(1)\n",
    "        self.edge_dim = train_data[0].edge_attr.size(1)\n",
    "        \n",
    "        # Allow fixed params across sessions\n",
    "        self.fixed_params = {}\n",
    "\n",
    "    def define_hyperparameter_space(self, trial, param_ranges=None):\n",
    "        \"\"\"Define hyperparameter search space, respecting fixed params and dynamic ranges.\"\"\"\n",
    "\n",
    "        if param_ranges is None:\n",
    "            param_ranges = {}\n",
    "\n",
    "        # Hidden dim - only suggest if not fixed\n",
    "        if 'hidden_dim' in self.fixed_params:\n",
    "            hidden_dim = self.fixed_params['hidden_dim']\n",
    "        else:\n",
    "            hidden_dim = trial.suggest_categorical('hidden_dim', param_ranges.get('hidden_dim', [64, 128]))\n",
    "\n",
    "        # Graph layers - only suggest if not fixed\n",
    "        if 'graph_layers' in self.fixed_params:\n",
    "            graph_layers = self.fixed_params['graph_layers']\n",
    "        else:\n",
    "            graph_layers = trial.suggest_int('graph_layers', *param_ranges.get('graph_layers', (2, 6)))\n",
    "\n",
    "        # FC layers - only suggest if not fixed\n",
    "        if 'fc_layers' in self.fixed_params:\n",
    "            fc_layers = self.fixed_params['fc_layers']\n",
    "        else:\n",
    "            fc_layers = trial.suggest_int('fc_layers', *param_ranges.get('fc_layers', (2, 6)))\n",
    "\n",
    "        # Learning rate - only suggest if not fixed\n",
    "        if 'lr' in self.fixed_params:\n",
    "            lr = self.fixed_params['lr']\n",
    "        else:\n",
    "            lr = trial.suggest_float('lr', *param_ranges.get('lr', (1e-6, 1e-2)), log=True)\n",
    "\n",
    "        # Weight decay - only suggest if not fixed\n",
    "        if 'weight_decay' in self.fixed_params:\n",
    "            weight_decay = self.fixed_params['weight_decay']\n",
    "        else:\n",
    "            weight_decay = trial.suggest_float('weight_decay', *param_ranges.get('weight_decay', (1e-6, 1e-2)), log=True)\n",
    "\n",
    "        # Batch size - only suggest if not fixed\n",
    "        if 'batch_size' in self.fixed_params:\n",
    "            batch_size = self.fixed_params['batch_size']\n",
    "        else:\n",
    "            batch_size = trial.suggest_categorical('batch_size', param_ranges.get('batch_size', [32, 64]))\n",
    "\n",
    "        # Physics loss scaling factors s1, s2, s3 - only suggest if not fixed\n",
    "        if 's1' in self.fixed_params:\n",
    "            s1 = self.fixed_params['s1']\n",
    "        else:\n",
    "            s1 = trial.suggest_categorical('s1', [1, 1e+1, 1e+2, 1e+3])\n",
    "\n",
    "        if 's2' in self.fixed_params:\n",
    "            s2 = self.fixed_params['s2']\n",
    "        else:\n",
    "            s2 = trial.suggest_categorical('s2', [1, 1e+1, 1e+2, 1e+3])\n",
    "\n",
    "        if 's3' in self.fixed_params:\n",
    "            s3 = self.fixed_params['s3']\n",
    "        else:\n",
    "            s3 = trial.suggest_categorical('s3', [1, 1e+1, 1e+2, 1e+3])\n",
    "\n",
    "\n",
    "        return {\n",
    "            'hidden_dim': hidden_dim,\n",
    "            'graph_layers': graph_layers,\n",
    "            'fc_layers': fc_layers,\n",
    "            'lr': lr,\n",
    "            'weight_decay': weight_decay,\n",
    "            'batch_size': batch_size,\n",
    "            's1': s1,\n",
    "            's2': s2,\n",
    "            's3': s3\n",
    "        }\n",
    "\n",
    "    def create_model(self, params):\n",
    "        model = ModelArchitecture.VLEAmineCO2(\n",
    "            node_dim=self.node_dim,\n",
    "            edge_dim=self.edge_dim,\n",
    "            hidden_dim=params['hidden_dim'],\n",
    "            graph_layers=params['graph_layers'],\n",
    "            fc_layers=params['fc_layers'],\n",
    "            use_adaptive_pooling=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def train_with_params(self, params, trial=None):\n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(self.train_data, batch_size=params['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(self.val_data, batch_size=params['batch_size'], shuffle=False)\n",
    "        \n",
    "        # Create model and optimizer\n",
    "        model = self.create_model(params)\n",
    "        criterion = ModelArchitecture.MSLELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), \n",
    "                               lr=params['lr'], \n",
    "                               weight_decay=params['weight_decay'])\n",
    "        \n",
    "        # Training parameters\n",
    "        num_epochs = 100\n",
    "        patience = 10\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        epochs_without_improvement = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            for data in train_loader:\n",
    "                data = data.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                \n",
    "                # Main loss\n",
    "                loss_main = criterion(output, data.aco2.view(-1, 1))\n",
    "                \n",
    "                # Physics losses\n",
    "                loss_thermo1 = ModelArchitecture.grad_pres(model, data, self.pco2_std, self.pco2_mean, params['s1']) if params['s1'] != 0 else 0.0\n",
    "                loss_thermo2 = ModelArchitecture.grad_temp(model, data, self.temp_std, self.temp_mean, params['s2']) if params['s2'] != 0 else 0.0\n",
    "                loss_thermo3 = ModelArchitecture.grad_conc(model, data, self.conc_std, self.conc_mean, params['s3']) if params['s3'] != 0 else 0.0\n",
    "                \n",
    "                total_loss = loss_main + loss_thermo1 + loss_thermo2 + loss_thermo3\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += total_loss.item()\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for data in val_loader:\n",
    "                    data = data.to(self.device)\n",
    "                    output = model(data)\n",
    "                    loss = criterion(output, data.aco2.view(-1, 1))\n",
    "                    val_loss += loss.item()\n",
    "            \n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            \n",
    "            # Early stopping and pruning\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "            \n",
    "            if trial is not None:\n",
    "                trial.report(avg_val_loss, epoch)\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "            if epochs_without_improvement >= patience:\n",
    "                break\n",
    "        \n",
    "        return best_val_loss\n",
    "    \n",
    "    def objective_with_ranges(self, trial, param_ranges=None):\n",
    "        try:\n",
    "            params = self.define_hyperparameter_space(trial, param_ranges=param_ranges)\n",
    "            val_loss = self.train_with_params(params, trial)\n",
    "            return val_loss\n",
    "        except Exception as e:\n",
    "            print(f\"Trial failed with error: {str(e)}\")\n",
    "            return float('inf')\n",
    "    \n",
    "    def optimize(self, study_name=\"gnn_hyperopt\", continue_from_last=True, \n",
    "                fixed_params=None, param_ranges=None):\n",
    "        \"\"\"\n",
    "        Run Bayesian hyperparameter optimization.\n",
    "\n",
    "        Args:\n",
    "            study_name (str): Name of the study.\n",
    "            continue_from_last (bool): If True, resume from previously saved study.\n",
    "            fixed_params (dict): Hyperparameters to keep constant.\n",
    "            param_ranges (dict): Custom search ranges for other hyperparameters.\n",
    "        Returns:\n",
    "            study: Optuna study object.\n",
    "            best_params_complete: Dictionary of best hyperparameters (fixed + tuned).\n",
    "        \"\"\"\n",
    "\n",
    "        # Update fixed params - this is the key fix\n",
    "        if fixed_params:\n",
    "            self.fixed_params.update(fixed_params)\n",
    "            print(f\"Fixed parameters: {self.fixed_params}\")\n",
    "\n",
    "        # Store param_ranges for objective\n",
    "        self._param_ranges = param_ranges if param_ranges is not None else {}\n",
    "\n",
    "        # Path to save study\n",
    "        study_path = f\"{self.parent_directory}/bayesian_optimization/study_{study_name}.pkl\"\n",
    "        os.makedirs(os.path.dirname(study_path), exist_ok=True)\n",
    "\n",
    "        sampler = TPESampler(seed=self.seed)\n",
    "        pruner = MedianPruner(n_startup_trials=10, n_warmup_steps=10, interval_steps=5)\n",
    "\n",
    "        # Load or create study\n",
    "        if continue_from_last and os.path.exists(study_path):\n",
    "            print(f\"Resuming study from {study_path}...\")\n",
    "            study = joblib.load(study_path)\n",
    "        else:\n",
    "            print(\"Creating a new study...\")\n",
    "            study = optuna.create_study(\n",
    "                direction='minimize',\n",
    "                sampler=sampler,\n",
    "                pruner=pruner,\n",
    "                study_name=study_name\n",
    "            )\n",
    "\n",
    "        # Wrap the objective to inject fixed params as trial user attributes\n",
    "        def objective(trial):\n",
    "            # Log fixed params in this trial\n",
    "            for k, v in self.fixed_params.items():\n",
    "                trial.set_user_attr(f\"fixed_{k}\", v)\n",
    "            return self.objective_with_ranges(trial, self._param_ranges)\n",
    "\n",
    "        # Run optimization\n",
    "        study.optimize(\n",
    "            objective,\n",
    "            n_trials=self.n_trials,\n",
    "            timeout=None,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "\n",
    "        # Save study\n",
    "        joblib.dump(study, study_path)\n",
    "        print(f\"Study saved at {study_path}\")\n",
    "        print(f\"Best validation loss: {study.best_value:.6f}\")\n",
    "\n",
    "        # Merge fixed params OVER Optuna's best params\n",
    "        best_params_complete = {**study.best_params, **self.fixed_params}\n",
    "\n",
    "        # Print the complete parameter set\n",
    "        print(\"Best parameters (including fixed):\")\n",
    "        for k, v in best_params_complete.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "\n",
    "        return study, best_params_complete\n",
    "        \n",
    "    def plot_optimization_results(self, study):\n",
    "        \"\"\"Visualize optimization progress and parameter importance.\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Optimization history\n",
    "        trial_values = [trial.value for trial in study.trials if trial.value is not None]\n",
    "        axes[0, 0].plot(trial_values, marker='o')\n",
    "        axes[0, 0].set_xlabel('Trial')\n",
    "        axes[0, 0].set_ylabel('Validation Loss')\n",
    "        axes[0, 0].set_title('Optimization History')\n",
    "        axes[0, 0].grid(True)\n",
    "        \n",
    "        # Parameter importance (if available)\n",
    "        try:\n",
    "            importance = optuna.importance.get_param_importances(study)\n",
    "            if importance:\n",
    "                params = list(importance.keys())\n",
    "                values = list(importance.values())\n",
    "                axes[0, 1].barh(params, values)\n",
    "                axes[0, 1].set_xlabel('Importance')\n",
    "                axes[0, 1].set_title('Parameter Importance')\n",
    "            else:\n",
    "                axes[0, 1].text(0.5, 0.5, 'Parameter importance\\nnot available', \n",
    "                                ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "        except:\n",
    "            axes[0, 1].text(0.5, 0.5, 'Parameter importance\\nnot available', \n",
    "                            ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "        \n",
    "        # Best trial convergence\n",
    "        best_values = []\n",
    "        best_so_far = float('inf')\n",
    "        for trial in study.trials:\n",
    "            if trial.value is not None and trial.value < best_so_far:\n",
    "                best_so_far = trial.value\n",
    "            best_values.append(best_so_far)\n",
    "        \n",
    "        axes[1, 0].plot(best_values, marker='o')\n",
    "        axes[1, 0].set_xlabel('Trial')\n",
    "        axes[1, 0].set_ylabel('Best Validation Loss')\n",
    "        axes[1, 0].set_title('Best Score Convergence')\n",
    "        axes[1, 0].grid(True)\n",
    "        \n",
    "        # Distribution of validation losses\n",
    "        valid_values = [v for v in trial_values if v != float('inf')]\n",
    "        if valid_values:\n",
    "            axes[1, 1].hist(valid_values, bins=20, alpha=0.7)\n",
    "            axes[1, 1].set_xlabel('Validation Loss')\n",
    "            axes[1, 1].set_ylabel('Frequency')\n",
    "            axes[1, 1].set_title('Distribution of Validation Losses')\n",
    "            axes[1, 1].grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD & GRAPH GENERATION\n",
    "df_components = load_dataset(get_path(file_name = 'components_set.csv', folder_name='datasets'))\n",
    "smiles_dict = dict(zip(df_components['Abbreviation'], df_components['SMILES']))\n",
    "df_systems = load_dataset(get_path(file_name = 'systems_set.csv', folder_name='datasets'))\n",
    "smiles_list = df_components[\"SMILES\"].dropna().tolist()\n",
    "mol_name_dict = smiles_dict.copy()\n",
    "# GRAPH\n",
    "system_graphs = process_dataset(df_systems, smiles_dict)\n",
    "# LOAD DATASET\n",
    "splitter_1 = DataSplitter(system_graphs, random_state=split_seed)\n",
    "RASset1, RASset2, RASset3 = splitter_1.rarity_aware_unseen_amine_split()\n",
    "opt_data = RASset1 + RASset2\n",
    "\n",
    "# HYBRID\n",
    "splitter_2 = DataSplitter(opt_data, random_state=split_seed)\n",
    "SRSset1, SRSset2, SRSset3 = splitter_2.stratified_random_split()\n",
    "train_data = SRSset1\n",
    "val_data = SRSset2 + SRSset3\n",
    "test_data = RASset3\n",
    "#Retrieve the statistics of train_data\n",
    "stats = compute_statistics(train_data)\n",
    "conc_mean = stats[0]\n",
    "conc_std = stats[1]\n",
    "temp_mean = stats[2]\n",
    "temp_std = stats[3]\n",
    "pco2_mean = stats[4]\n",
    "pco2_std = stats[5]\n",
    "#Apply the scaling to validation and test\n",
    "original_train_data = copy.deepcopy(train_data)\n",
    "original_val_data = copy.deepcopy(val_data)\n",
    "original_test_data = copy.deepcopy(test_data)\n",
    "combined_original_data = original_train_data + original_val_data + original_test_data\n",
    "train_data = scale_graphs(train_data, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    "val_data = scale_graphs(val_data, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    "test_data = scale_graphs(test_data, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIAL SEARCH\n",
    "optimizer = BayesianOptimizer(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    conc_std=conc_std,\n",
    "    conc_mean=conc_mean,\n",
    "    temp_std=temp_std,\n",
    "    temp_mean=temp_mean,\n",
    "    pco2_std=pco2_std,\n",
    "    pco2_mean=pco2_mean,\n",
    "    device=device,\n",
    "    parent_directory=parent_directory,\n",
    "    n_trials=20,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "study, best_params = optimizer.optimize(study_name=f\"{study_name}_{runtyp}\",\n",
    "                                        continue_from_last=False)\n",
    "\n",
    "# Plot results\n",
    "optimizer.plot_optimization_results(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAR-UP CUDA MEMORY\n",
    "del optimizer\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2ND SEARCH\n",
    "optimizer = BayesianOptimizer(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    conc_std=conc_std,\n",
    "    conc_mean=conc_mean,\n",
    "    temp_std=temp_std,\n",
    "    temp_mean=temp_mean,\n",
    "    pco2_std=pco2_std,\n",
    "    pco2_mean=pco2_mean,\n",
    "    device=device,\n",
    "    parent_directory=parent_directory,\n",
    "    n_trials=20,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "study, best_params = optimizer.optimize(study_name=f\"{study_name}_{runtyp}\",\n",
    "                                        continue_from_last=True)\n",
    "\n",
    "# Plot results\n",
    "optimizer.plot_optimization_results(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAR-UP CUDA MEMORY\n",
    "del optimizer\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3RD SEARCH\n",
    "optimizer = BayesianOptimizer(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    conc_std=conc_std,\n",
    "    conc_mean=conc_mean,\n",
    "    temp_std=temp_std,\n",
    "    temp_mean=temp_mean,\n",
    "    pco2_std=pco2_std,\n",
    "    pco2_mean=pco2_mean,\n",
    "    device=device,\n",
    "    parent_directory=parent_directory,\n",
    "    n_trials=20,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "study, best_params = optimizer.optimize(study_name=f\"{study_name}_{runtyp}\",\n",
    "                                        continue_from_last=True)\n",
    "\n",
    "# Plot results\n",
    "optimizer.plot_optimization_results(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAR-UP CUDA MEMORY\n",
    "del optimizer\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4TH SEARCH\n",
    "optimizer = BayesianOptimizer(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    conc_std=conc_std,\n",
    "    conc_mean=conc_mean,\n",
    "    temp_std=temp_std,\n",
    "    temp_mean=temp_mean,\n",
    "    pco2_std=pco2_std,\n",
    "    pco2_mean=pco2_mean,\n",
    "    device=device,\n",
    "    parent_directory=parent_directory,\n",
    "    n_trials=20,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "study, best_params = optimizer.optimize(study_name=f\"{study_name}_{runtyp}\",\n",
    "                                        continue_from_last=True)\n",
    "\n",
    "# Plot results\n",
    "optimizer.plot_optimization_results(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAR-UP CUDA MEMORY\n",
    "del optimizer\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # 5TH SEARCH\n",
    "optimizer = BayesianOptimizer(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    conc_std=conc_std,\n",
    "    conc_mean=conc_mean,\n",
    "    temp_std=temp_std,\n",
    "    temp_mean=temp_mean,\n",
    "    pco2_std=pco2_std,\n",
    "    pco2_mean=pco2_mean,\n",
    "    device=device,\n",
    "    parent_directory=parent_directory,\n",
    "    n_trials=20,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "study, best_params = optimizer.optimize(study_name=f\"{study_name}_{runtyp}\",\n",
    "                                        continue_from_last=True)\n",
    "\n",
    "# Plot results\n",
    "optimizer.plot_optimization_results(study) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # CLEAR-UP CUDA MEMORY\n",
    "del optimizer\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # FINE TUNING\n",
    "optimizer = BayesianOptimizer(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    conc_std=conc_std,\n",
    "    conc_mean=conc_mean,\n",
    "    temp_std=temp_std,\n",
    "    temp_mean=temp_mean,\n",
    "    pco2_std=pco2_std,\n",
    "    pco2_mean=pco2_mean,\n",
    "    device=device,\n",
    "    parent_directory=parent_directory,\n",
    "    n_trials=150,\n",
    "    seed=seed\n",
    ")\n",
    "# FINE TUNING\n",
    "fixed_params = {\"hidden_dim\"    : 128, \n",
    "                \"graph_layers\"  : 2, \n",
    "                \"fc_layers\"     : 4,\n",
    "                \"batch_size\"    : 32}\n",
    "\n",
    "# Change search range for others\n",
    "param_ranges = {\n",
    "    \"s1\": (10, 100000),\n",
    "    \"s2\": (10, 100000),\n",
    "    \"s3\": (10, 100000)\n",
    "}\n",
    "\n",
    "study, best_params = optimizer.optimize(\n",
    "    study_name=f\"{study_name}_{runtyp}\",\n",
    "    continue_from_last=True,\n",
    "    fixed_params=fixed_params,\n",
    "    param_ranges=param_ranges\n",
    ")\n",
    "print(f\"Total trials now: {len(study.trials)}\")\n",
    "# Plot results\n",
    "optimizer.plot_optimization_results(study) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST HYPERPARAMETERS\n",
    "hidden_dim = best_params['hidden_dim']\n",
    "graph_layers = best_params['graph_layers']\n",
    "fc_layers = best_params['fc_layers']\n",
    "lr = best_params['lr']\n",
    "weight_decay = best_params['weight_decay']\n",
    "batch_size = best_params['batch_size']\n",
    "s1 = best_params['s1']\n",
    "s2 = best_params['s2']\n",
    "s3 = best_params['s3']\n",
    "print(\"Optimzed hyperparameters loaded...\")\n",
    "print(f\"hidden_dim: {hidden_dim}\")\n",
    "print(f\"graph_layers: {graph_layers}\")\n",
    "print(f\"fc_layers: {fc_layers}\")\n",
    "print(f\"lr: {lr}\")\n",
    "print(f\"weight_decay: {weight_decay}\")\n",
    "print(f\"batch_size: {batch_size}\")\n",
    "print(f\"s1: {s1}\")\n",
    "print(f\"s2: {s2}\")\n",
    "print(f\"s3: {s3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION -> TRAINING LOOP\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, \n",
    "                    pco2_std, temp_std, conc_std,\n",
    "                    pco2_mean, temp_mean, conc_mean,\n",
    "                    s1, s2, s3, \n",
    "                    use_physics_in_loss=True, monitor_physics=True):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_main_loss = 0\n",
    "    total_thermo1_loss = 0\n",
    "    total_thermo2_loss = 0\n",
    "    total_thermo3_loss = 0\n",
    "    \n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        loss_main = criterion(output, data.aco2.view(-1, 1))\n",
    "        \n",
    "        # Calculate physics losses\n",
    "        if use_physics_in_loss or monitor_physics:\n",
    "            loss_thermo1 = ModelArchitecture.grad_pres(model, data, pco2_std, pco2_mean, s1) if s1 != 0 else 0.0\n",
    "            loss_thermo2 = ModelArchitecture.grad_temp(model, data, temp_std, temp_mean, s2) if s2 != 0 else 0.0\n",
    "            loss_thermo3 = ModelArchitecture.grad_conc(model, data, conc_std, conc_mean, s3) if s3 != 0 else 0.0\n",
    "        else:\n",
    "            loss_thermo1 = loss_thermo2 = loss_thermo3 = 0.0\n",
    "        \n",
    "        # Only include physics losses in backprop if use_physics_in_loss=True\n",
    "        if use_physics_in_loss:\n",
    "            loss = loss_main + loss_thermo1 + loss_thermo2 + loss_thermo3\n",
    "        else:\n",
    "            loss = loss_main\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track total loss consistently with what was actually backpropagated\n",
    "        if use_physics_in_loss:\n",
    "            total_loss += loss.item()  # This includes physics losses\n",
    "        else:\n",
    "            total_loss += loss_main.item()\n",
    "        total_main_loss += loss_main.item()\n",
    "        total_thermo1_loss += loss_thermo1.item() if isinstance(loss_thermo1, torch.Tensor) else loss_thermo1\n",
    "        total_thermo2_loss += loss_thermo2.item() if isinstance(loss_thermo2, torch.Tensor) else loss_thermo2\n",
    "        total_thermo3_loss += loss_thermo3.item() if isinstance(loss_thermo3, torch.Tensor) else loss_thermo3\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    avg_main_loss = total_main_loss / len(train_loader)\n",
    "    avg_thermo1_loss = total_thermo1_loss / len(train_loader)\n",
    "    avg_thermo2_loss = total_thermo2_loss / len(train_loader)\n",
    "    avg_thermo3_loss = total_thermo3_loss / len(train_loader)\n",
    "    \n",
    "    return avg_train_loss, avg_main_loss, avg_thermo1_loss, avg_thermo2_loss, avg_thermo3_loss\n",
    "\n",
    "\n",
    "# Validation Step\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data.aco2.view(-1, 1))\n",
    "            val_loss += loss.item()\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    return avg_val_loss\n",
    "\n",
    "# Test Evaluation (Metrics)\n",
    "def evaluate_test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_outputs = []\n",
    "    test_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data.aco2.view(-1, 1))\n",
    "            test_loss += loss.item()\n",
    "            test_outputs.append(output.cpu().numpy())\n",
    "            test_targets.append(data.aco2.view(-1, 1).cpu().numpy())\n",
    "    \n",
    "    test_outputs = np.concatenate(test_outputs, axis=0)\n",
    "    test_targets = np.concatenate(test_targets, axis=0)\n",
    "    test_r2 = r2_score(test_targets, test_outputs)\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    return avg_test_loss, test_r2\n",
    "# Loss Plotting\n",
    "def plot_losses(train_losses, val_losses, test_losses, test_r2_scores, \n",
    "                main_losses, thermo1_losses, thermo2_losses, thermo3_losses):\n",
    "    # Original combined loss plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Total Train Loss')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "    plt.plot(range(1, len(test_losses) + 1), test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training, Validation and Test Loss Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(range(1, len(test_r2_scores) + 1), test_r2_scores, color='green')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('R² Score')\n",
    "    plt.title('Test R² Score per Epoch')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Individual physics loss components\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(range(1, len(main_losses) + 1), main_losses, label='Main Loss', color='blue')\n",
    "    plt.plot(range(1, len(thermo1_losses) + 1), thermo1_losses, label='Partial pressure gradient (s1)', color='red')\n",
    "    plt.plot(range(1, len(thermo2_losses) + 1), thermo2_losses, label='Temperature gradient (s2)', color='orange')\n",
    "    plt.plot(range(1, len(thermo3_losses) + 1), thermo3_losses, label='Concentration gradient (s3)', color='purple')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Individual Loss Components')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Physics losses only (zoomed view)\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(range(1, len(thermo1_losses) + 1), thermo1_losses, label='Partial pressure gradient (s1)', color='red')\n",
    "    plt.plot(range(1, len(thermo2_losses) + 1), thermo2_losses, label='Temperature gradient (s2)', color='orange')\n",
    "    plt.plot(range(1, len(thermo3_losses) + 1), thermo3_losses, label='Concentration gradient (s3)', color='purple')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Physics Loss Components (Zoomed)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main Training Loop\n",
    "def train_model(model, train_loader, val_loader, \n",
    "                test_loader, criterion, optimizer, \n",
    "                pco2_std, temp_std, conc_std,\n",
    "                pco2_mean, temp_mean, conc_mean,\n",
    "                s1, s2, s3,\n",
    "                device, num_epochs, \n",
    "                seed, parent_directory,\n",
    "                use_physics_in_loss=True, monitor_physics=True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        use_physics_in_loss (bool): Whether to include physics losses in backpropagation\n",
    "        monitor_physics (bool): Whether to compute and track physics losses for monitoring\n",
    "    \"\"\"\n",
    "    # Initialize loss tracking lists\n",
    "    train_losses, val_losses, test_losses, test_r2_scores = [], [], [], []\n",
    "    main_losses, thermo1_losses, thermo2_losses, thermo3_losses = [], [], [], []\n",
    "\n",
    "    # Print training mode\n",
    "    if use_physics_in_loss and monitor_physics:\n",
    "        print(\"Training with physics losses in backpropagation + monitoring\")\n",
    "    elif monitor_physics and not use_physics_in_loss:\n",
    "        print(\"Training with Main Loss only, but monitoring physics losses\")\n",
    "    elif use_physics_in_loss and not monitor_physics:\n",
    "        print(\"Training with physics losses in backpropagation (no monitoring)\")\n",
    "    else:\n",
    "        print(\"Training with Main Loss only (no physics)\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Get individual loss components from training\n",
    "        avg_train_loss, avg_main_loss, avg_thermo1_loss, avg_thermo2_loss, avg_thermo3_loss = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, device, \n",
    "            pco2_std, temp_std, conc_std, \n",
    "            pco2_mean, temp_mean, conc_mean,\n",
    "            s1, s2, s3, use_physics_in_loss, monitor_physics)\n",
    "\n",
    "        avg_val_loss = validate(model, val_loader, criterion, device)\n",
    "        avg_test_loss, test_r2 = evaluate_test(model, test_loader, criterion, device)\n",
    "\n",
    "        # Store all losses\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        test_r2_scores.append(test_r2)\n",
    "        main_losses.append(avg_main_loss)\n",
    "        thermo1_losses.append(avg_thermo1_loss)\n",
    "        thermo2_losses.append(avg_thermo2_loss)\n",
    "        thermo3_losses.append(avg_thermo3_loss)\n",
    "\n",
    "        # Enhanced print statement with individual loss components\n",
    "        if monitor_physics:\n",
    "            physics_info = f'Main Loss: {round(avg_main_loss, 4)}, gradP: {round(avg_thermo1_loss, 4)}, gradT: {round(avg_thermo2_loss, 4)}, gradC: {round(avg_thermo3_loss, 4)}'\n",
    "            physics_status = \" [BACKPROP]\" if use_physics_in_loss else \" [MONITOR]\"\n",
    "        else:\n",
    "            physics_info = f'Main Loss: {round(avg_main_loss, 4)}'\n",
    "            physics_status = \"\"\n",
    "            \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Total Loss: {round(avg_train_loss, 4)}, '\n",
    "              f'Val Loss: {round(avg_val_loss, 4)}, Test Loss: {round(avg_test_loss, 4)}, '\n",
    "              f'Test R2: {round(test_r2, 4)} | {physics_info}{physics_status}')\n",
    "    # Plot all losses including individual components\n",
    "    plot_losses(train_losses, val_losses, test_losses, test_r2_scores, \n",
    "                main_losses, thermo1_losses, thermo2_losses, thermo3_losses)\n",
    "    \n",
    "    # Return all tracked losses\n",
    "    return (train_losses, val_losses, test_losses, test_r2_scores, \n",
    "            main_losses, thermo1_losses, thermo2_losses, thermo3_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN WITH OPTIMIZED HYPERPARAMETERS\n",
    "print(\"Training final model with optimized hyperparameters...\")\n",
    "\n",
    "# Load the data into DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "node_dim = train_data[0].x.size(1)\n",
    "edge_dim = train_data[0].edge_attr.size(1)\n",
    "\n",
    "model = ModelArchitecture.VLEAmineCO2(node_dim=node_dim,\n",
    "                    edge_dim=edge_dim, \n",
    "                    hidden_dim=hidden_dim,\n",
    "                    graph_layers=graph_layers,\n",
    "                    fc_layers=fc_layers,\n",
    "                    use_adaptive_pooling=True\n",
    "                    ).to(device)\n",
    "\n",
    "criterion = ModelArchitecture.MSLELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "# Start timing\n",
    "real_time_start = time.time()  # Real time (wall-clock time)\n",
    "cpu_time_start = time.process_time()  # CPU time\n",
    "\n",
    "# Updated to receive all loss components\n",
    "(train_losses, val_losses, test_losses, test_r2_scores, \n",
    " main_losses, thermo1_losses, thermo2_losses, thermo3_losses) = train_model(\n",
    "    model, train_loader, val_loader, test_loader, \n",
    "    criterion, optimizer, \n",
    "    pco2_std, temp_std, conc_std, \n",
    "    pco2_mean, temp_mean, conc_mean,\n",
    "    s1, s2, s3, \n",
    "    device, num_epochs=num_epochs,\n",
    "    seed=seed, parent_directory=parent_directory,\n",
    "    use_physics_in_loss=use_physics_in_loss, monitor_physics=monitor_physics)\n",
    "\n",
    "model_path = f\"{parent_directory}/models/models_root/{runtyp}/model_weights/MODEL_{run_time}.pth\"\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "torch.save(model.state_dict(), model_path)\n",
    "# Enhanced CSV path with individual loss tracking\n",
    "csv_path = f\"{parent_directory}/models/models_root/{runtyp}/losses/losses_{run_time}.csv\"\n",
    "os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "\n",
    "# Write comprehensive loss data to CSV\n",
    "with open(csv_path, mode='w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    # Write enhanced header with individual loss components\n",
    "    writer.writerow([\n",
    "        \"Epoch\", \"Train Loss\", \"Validation Loss\", \"Test Loss\", \"Test R2\",\n",
    "        \"Main Loss\", \"gradP Loss (s1)\", \"gradT Loss (s2)\", \"gradC Loss (s3)\"\n",
    "    ])\n",
    "    \n",
    "    # Write losses for each epoch including individual components (training)\n",
    "    for epoch in range(len(train_losses)):\n",
    "        writer.writerow([\n",
    "            epoch + 1,\n",
    "            train_losses[epoch],\n",
    "            val_losses[epoch],\n",
    "            test_losses[epoch],\n",
    "            test_r2_scores[epoch],\n",
    "            main_losses[epoch],\n",
    "            thermo1_losses[epoch],\n",
    "            thermo2_losses[epoch],\n",
    "            thermo3_losses[epoch]\n",
    "        ])\n",
    "\n",
    "# Optional: Save individual physics loss components to separate CSV for detailed analysis\n",
    "physics_csv_path = f\"{parent_directory}/models/models_root/{runtyp}/physics_loss/physics_losses_{run_time}.csv\"\n",
    "os.makedirs(os.path.dirname(physics_csv_path), exist_ok=True)\n",
    "with open(physics_csv_path, mode='w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Epoch\", \"Main Loss\", \"gradP Loss (s1)\", \"gradT Loss (s2)\", \"gradC Loss (s3)\", \"Physics Loss Sum\"])\n",
    "    \n",
    "    for epoch in range(len(main_losses)):\n",
    "        physics_sum = thermo1_losses[epoch] + thermo2_losses[epoch] + thermo3_losses[epoch]\n",
    "        writer.writerow([\n",
    "            epoch + 1,\n",
    "            main_losses[epoch],\n",
    "            thermo1_losses[epoch],\n",
    "            thermo2_losses[epoch],\n",
    "            thermo3_losses[epoch],\n",
    "            physics_sum\n",
    "        ])\n",
    "\n",
    "# End timing\n",
    "real_time_end = time.time()\n",
    "cpu_time_end = time.process_time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "real_time_elapsed = real_time_end - real_time_start\n",
    "cpu_time_elapsed = cpu_time_end - cpu_time_start\n",
    "\n",
    "# Output the training time\n",
    "print(f\"Training Real Time (Wall-Clock Time): {real_time_elapsed:.2f} seconds\")\n",
    "print(f\"Training CPU Time: {cpu_time_elapsed:.2f} seconds\")\n",
    "\n",
    "# Print summary statistics of loss components\n",
    "print(\"\\n=== Training Summary ===\")\n",
    "print(f\"Final Total Train Loss: {train_losses[-1]:.6f}\")\n",
    "print(f\"Final Validation Loss: {val_losses[-1]:.6f}\")\n",
    "print(f\"Final Test Loss: {test_losses[-1]:.6f}\")\n",
    "print(f\"Final Test R2: {test_r2_scores[-1]:.6f}\")\n",
    "print(f\"Final Main Loss: {main_losses[-1]:.6f}\")\n",
    "print(f\"Final gradP (s1): {thermo1_losses[-1]:.6f}\")\n",
    "print(f\"Final gradT (s2): {thermo2_losses[-1]:.6f}\")\n",
    "print(f\"Final gradC (s3): {thermo3_losses[-1]:.6f}\")\n",
    "print(f\"\\nLoss data saved to: {csv_path}\")\n",
    "print(f\"Physics loss details saved to: {physics_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARITY PLOT GENERATION\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import torch\n",
    "\n",
    "def collect_predictions_and_true_values(model, data_loader, device):\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            predictions.extend(output.cpu().numpy())\n",
    "            true_values.extend(data.aco2.cpu().numpy())\n",
    "    \n",
    "    return predictions, true_values\n",
    "\n",
    "# Function to calculate R² and RMSE\n",
    "def calculate_metrics(true_values, predictions):\n",
    "    r2 = r2_score(true_values, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predictions))\n",
    "    return r2, rmse\n",
    "\n",
    "# Function to save metrics to CSV\n",
    "def save_metrics_to_csv(r2_train, rmse_train, r2_val, rmse_val, r2_test, rmse_test, parent_directory):\n",
    "    # Create the metrics dictionary\n",
    "    metrics_data = {\n",
    "        'Dataset': ['Training', 'Validation', 'Test'],\n",
    "        'R2': [r2_train, r2_val, r2_test],\n",
    "        'RMSE': [rmse_train, rmse_val, rmse_test]\n",
    "    }\n",
    "\n",
    "# Function to plot the parity plot with marginal histograms\n",
    "def plot_parity_plot(train_true_values, train_predictions, \n",
    "                     val_true_values, val_predictions, \n",
    "                     test_true_values, test_predictions,\n",
    "                     parent_directory=None):\n",
    "    fontsize = 16\n",
    "    matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "    # Calculate metrics\n",
    "    r2_train, rmse_train = calculate_metrics(train_true_values, train_predictions)\n",
    "    r2_val, rmse_val = calculate_metrics(val_true_values, val_predictions)\n",
    "    r2_test, rmse_test = calculate_metrics(test_true_values, test_predictions)\n",
    "\n",
    "    # Create figure with gridspec for histograms\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    gs = gridspec.GridSpec(2, 2, width_ratios=[4, 1], height_ratios=[1, 4], \n",
    "                          hspace=0.00, wspace=0.00)\n",
    "    \n",
    "    # Main plot\n",
    "    ax = fig.add_subplot(gs[1, 0])\n",
    "    ax_histx = fig.add_subplot(gs[0, 0], sharex=ax)\n",
    "    ax_histy = fig.add_subplot(gs[1, 1], sharey=ax)\n",
    "\n",
    "    # Scatter plots (keeping your exact style)\n",
    "    ax.scatter(train_true_values, train_predictions, \n",
    "                edgecolors='b', alpha=0.5, c='b', marker='o', \n",
    "                label=f'Train   (R² = {r2_train:.3f},  RMSE = {rmse_train:.3f})')\n",
    "    ax.scatter(val_true_values, val_predictions, \n",
    "                edgecolors='g', alpha=0.5, c='g', marker='^', \n",
    "                label=f'Val      (R² = {r2_val:.3f},  RMSE = {rmse_val:.3f})')\n",
    "    ax.scatter(test_true_values, test_predictions, \n",
    "                edgecolors='r', alpha=0.5, c='r', marker='v', \n",
    "                label=f'Test     (R² = {r2_test:.3f},  RMSE = {rmse_test:.3f})')\n",
    "\n",
    "    # Parity line (keeping your exact style)\n",
    "    max_val = max(max(train_true_values), max(val_true_values), max(test_true_values))\n",
    "    ax.plot([-0.1, max_val+0.5], [-0.1, max_val+0.5], '--', linewidth=1.5, color='black')\n",
    "\n",
    "    # Labels & ticks (keeping your exact formatting)\n",
    "    ax.set_xlabel('Actual Solubility', fontsize=fontsize)\n",
    "    ax.set_ylabel('Predicted Solubility', fontsize=fontsize)\n",
    "    ax.set_xlim(-0.1, 2.5)\n",
    "    ax.set_ylim(-0.1, 2.5)\n",
    "    ax.tick_params(axis='both', which='major', length=6, width=0.8, labelsize=fontsize)\n",
    "    ax.tick_params(axis='both', which='minor', length=4, width=0.8)\n",
    "    ax.minorticks_on()\n",
    "    ax.legend(fontsize=fontsize-3, loc='upper left', frameon=False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)    \n",
    "\n",
    "    # Add histograms with dataset differentiation\n",
    "    bins = np.linspace(-0.1, 2.5, 27)\n",
    "    \n",
    "    # Top histogram (experimental values) - stacked by dataset\n",
    "    ax_histx.hist([np.array(train_true_values).flatten(), \n",
    "                   np.array(val_true_values).flatten(), \n",
    "                   np.array(test_true_values).flatten()], \n",
    "                  bins=bins, color=['b', 'g', 'r'], \n",
    "                  alpha=0.5, stacked=True, edgecolor='black', linewidth=0.5)\n",
    "    ax_histx.tick_params(labelbottom=False, labelleft=False, left=False)\n",
    "    ax_histx.spines['top'].set_visible(False)\n",
    "    ax_histx.spines['right'].set_visible(False)\n",
    "    ax_histx.spines['left'].set_visible(False)\n",
    "    #ax_histx.spines['bottom'].set_visible(False)\n",
    "\n",
    "    # Right histogram (predicted values) - stacked by dataset\n",
    "    ax_histy.hist([np.array(train_predictions).flatten(), \n",
    "                   np.array(val_predictions).flatten(), \n",
    "                   np.array(test_predictions).flatten()], \n",
    "                  bins=bins, orientation='horizontal', color=['b', 'g', 'r'], \n",
    "                  alpha=0.5, stacked=True, edgecolor='black', linewidth=0.5)\n",
    "    ax_histy.tick_params(labelbottom=False, labelleft=False, bottom=False)\n",
    "    ax_histy.spines['top'].set_visible(False)\n",
    "    ax_histy.spines['right'].set_visible(False)\n",
    "    #ax_histy.spines['left'].set_visible(False)\n",
    "    ax_histy.spines['bottom'].set_visible(False)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Save metrics if needed\n",
    "    if parent_directory:\n",
    "        save_metrics_to_csv(r2_train, rmse_train, r2_val, rmse_val, r2_test, rmse_test, parent_directory)\n",
    "\n",
    "\n",
    "# Collect predictions and true values for training, validation, and test data\n",
    "train_predictions, train_true_values = collect_predictions_and_true_values(model, train_loader, device)\n",
    "val_predictions, val_true_values = collect_predictions_and_true_values(model, val_loader, device)\n",
    "test_predictions, test_true_values = collect_predictions_and_true_values(model, test_loader, device)\n",
    "\n",
    "# Plot the parity plot\n",
    "plot_parity_plot(train_true_values, train_predictions, \n",
    "                 val_true_values, val_predictions, \n",
    "                 test_true_values, test_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
