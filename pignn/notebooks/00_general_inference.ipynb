{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING LIBRARIES\n",
    "import copy\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'  # or ':16:8'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdmolops\n",
    "from rdkit.Chem import rdDistGeom as molDG\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem.rdchem import GetPeriodicTable\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch_geometric.nn import MessagePassing, GCNConv, global_mean_pool, GATConv\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch_geometric.data import Data, Dataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn.inits import reset\n",
    "import networkx as nx\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "# FUNCTIONS\n",
    "from data_processing import load_dataset, smiles_to_graph, process_dataset, generate_graphs\n",
    "from path_helpers import get_path\n",
    "from stats_compute import compute_statistics, scale_graphs\n",
    "from mol_visualize import recon_3d, viz_3d\n",
    "from smart_loader import load_model_for_inference\n",
    "import ModelArchitecture\n",
    "from EnhancedDataSplit import DataSplitter\n",
    "from collections import defaultdict\n",
    "from typing import Tuple, List\n",
    "# DIRECTORY SETUP\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETER SETTINGS\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# Reproducibility settings\n",
    "seed = 21\n",
    "split_seed = 42\n",
    "batch_size = 32\n",
    "runtime = timestamp\n",
    "selected_device = 'cuda' # either 'cuda' or 'cpu\n",
    "device = torch.device(selected_device)\n",
    "\n",
    "# CUDA Deterministic (ON/OFF SETTING)\n",
    "# For PyTorch\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.use_deterministic_algorithms(False)\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print('device           :', device)\n",
    "print('seed             :', seed)\n",
    "print('split seed       :', split_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARITY PLOT GENERATION (MEDIAN MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inference_dir = os.path.join(\n",
    "    os.path.dirname(os.getcwd()),\n",
    "    \"models\",\n",
    "    \"models_root\",\n",
    "    \"model_for_inference\",\n",
    "    \"ras_baseline\"\n",
    ")\n",
    "# List files and pick the first one\n",
    "files = sorted(os.listdir(model_inference_dir))  # sorted to make it deterministic\n",
    "if len(files) == 0:\n",
    "    raise FileNotFoundError(f\"No files found in {model_inference_dir}\")\n",
    "model_file = files[1]  # first file in directory\n",
    "print(f\"Using model file: {model_file}\")\n",
    "# Full path\n",
    "path = os.path.join(model_inference_dir, model_file)\n",
    "########### IMPORTING MODEL ###############\n",
    "selected_device = 'cuda'\n",
    "model = load_model_for_inference(path, device=device)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD & GRAPH GENERATION FOR EITHER SRS OR RAS\n",
    "df_components = load_dataset(get_path(file_name = 'components_set.csv', folder_name='datasets'))\n",
    "smiles_dict = dict(zip(df_components['Abbreviation'], df_components['SMILES']))\n",
    "df_systems = load_dataset(get_path(file_name = 'systems_set.csv', folder_name='datasets'))\n",
    "smiles_list = df_components[\"SMILES\"].dropna().tolist()\n",
    "mol_name_dict = smiles_dict.copy()\n",
    "# GRAPH\n",
    "system_graphs = process_dataset(df_systems, smiles_dict)\n",
    "# LOAD DATASET\n",
    "splitter = DataSplitter(system_graphs, random_state=split_seed)\n",
    "splitter.print_dataset_stats()\n",
    "# Options: rarity_aware_unseen_amine_split stratified_random_split\n",
    "train_data, val_data, test_data = splitter.rarity_aware_unseen_amine_split()\n",
    "#Retrieve the statistics of train_data\n",
    "stats = compute_statistics(train_data)\n",
    "conc_mean = stats[0]\n",
    "conc_std = stats[1]\n",
    "temp_mean = stats[2]\n",
    "temp_std = stats[3]\n",
    "pco2_mean = stats[4]\n",
    "pco2_std = stats[5]\n",
    "#Apply the scaling to validation and test\n",
    "original_train_data = copy.deepcopy(train_data)\n",
    "original_val_data = copy.deepcopy(val_data)\n",
    "original_test_data = copy.deepcopy(test_data)\n",
    "combined_original_data = original_train_data + original_val_data + original_test_data\n",
    "train_data = scale_graphs(train_data, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    "val_data = scale_graphs(val_data, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    "test_data = scale_graphs(test_data, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    "#Load the data into DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing varience of the alpha_co2 on datasets\n",
    "def get_alpha_stats(dataset):\n",
    "    values = torch.cat([torch.tensor([data.aco2]) for data in dataset])\n",
    "    \n",
    "    # Variance calculations\n",
    "    sample_var = torch.var(values, unbiased=True)      # N-1\n",
    "    population_var = torch.var(values, unbiased=False) # N\n",
    "    \n",
    "    # Range and statistics\n",
    "    min_val = torch.min(values)\n",
    "    max_val = torch.max(values)\n",
    "    mean_val = torch.mean(values)\n",
    "    std_val = torch.std(values, unbiased=True)\n",
    "    median_val = torch.median(values)\n",
    "    \n",
    "    return {\n",
    "        'sample_var': sample_var.item(),\n",
    "        'population_var': population_var.item(),\n",
    "        'min': min_val.item(),\n",
    "        'max': max_val.item(),\n",
    "        'range': (max_val - min_val).item(),\n",
    "        'mean': mean_val.item(),\n",
    "        'std': std_val.item(),\n",
    "        'median': median_val.item(),\n",
    "        'count': len(values)\n",
    "    }\n",
    "\n",
    "# Calculate statistics for all datasets\n",
    "train_stats = get_alpha_stats(original_train_data)\n",
    "val_stats = get_alpha_stats(original_val_data)\n",
    "test_stats = get_alpha_stats(original_test_data)\n",
    "\n",
    "# Print comprehensive statistics\n",
    "print(\"=\" * 80)\n",
    "print(\"Î±_CO2 DATASET STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "datasets = [('Train', train_stats), ('Val', val_stats), ('Test', test_stats)]\n",
    "\n",
    "for name, stats in datasets:\n",
    "    print(f\"\\n{name:>5} Dataset (n={stats['count']}):\")\n",
    "    print(f\"  Range:     [{stats['min']:.4f}, {stats['max']:.4f}]  (span: {stats['range']:.4f})\")\n",
    "    print(f\"  Mean:      {stats['mean']:.4f}  Â±  {stats['std']:.4f}\")\n",
    "    print(f\"  Median:    {stats['median']:.4f}\")\n",
    "    print(f\"  Variance:  {stats['sample_var']:.6f} (sample), {stats['population_var']:.6f} (population)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RELATIVE COMPARISONS (vs Train)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for name, stats in datasets[1:]:  # Skip train for comparison\n",
    "    print(f\"\\n{name} vs Train:\")\n",
    "    print(f\"  Range ratio:    {stats['range']/train_stats['range']:.3f}\")\n",
    "    print(f\"  Mean ratio:     {stats['mean']/train_stats['mean']:.3f}\")\n",
    "    print(f\"  Variance ratio: {stats['sample_var']/train_stats['sample_var']:.3f}\")\n",
    "    print(f\"  Min overlap:    {'Yes' if stats['min'] >= train_stats['min'] else 'No'} ({stats['min']:.4f} vs {train_stats['min']:.4f})\")\n",
    "    print(f\"  Max overlap:    {'Yes' if stats['max'] <= train_stats['max'] else 'No'} ({stats['max']:.4f} vs {train_stats['max']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # LOAD DATASET FOR RASHYB\n",
    "df_components = load_dataset(get_path(file_name = 'components_set.csv', folder_name='datasets'))\n",
    "smiles_dict = dict(zip(df_components['Abbreviation'], df_components['SMILES']))\n",
    "df_systems = load_dataset(get_path(file_name = 'systems_set.csv', folder_name='datasets'))\n",
    "smiles_list = df_components[\"SMILES\"].dropna().tolist()\n",
    "mol_name_dict = smiles_dict.copy()\n",
    "# GRAPH\n",
    "system_graphs = process_dataset(df_systems, smiles_dict)\n",
    "splitter_1 = DataSplitter(system_graphs, random_state=split_seed)\n",
    "RASset1, RASset2, RASset3 = splitter_1.rarity_aware_unseen_amine_split()\n",
    "opt_data = RASset1 + RASset2\n",
    "\n",
    "# HYBRID\n",
    "splitter_2 = DataSplitter(opt_data, random_state=split_seed)\n",
    "SRSset1, SRSset2, SRSset3 = splitter_2.stratified_random_split()\n",
    "train_data = SRSset1\n",
    "val_data = SRSset2 + SRSset3\n",
    "test_data = RASset3\n",
    "#Retrieve the statistics of train_data\n",
    "stats = compute_statistics(train_data)\n",
    "conc_mean = stats[0]\n",
    "conc_std = stats[1]\n",
    "temp_mean = stats[2]\n",
    "temp_std = stats[3]\n",
    "pco2_mean = stats[4]\n",
    "pco2_std = stats[5]\n",
    "#Apply the scaling to validation and test\n",
    "original_train_data = copy.deepcopy(train_data)\n",
    "original_val_data = copy.deepcopy(val_data)\n",
    "original_test_data = copy.deepcopy(test_data)\n",
    "combined_original_data = original_train_data + original_val_data + original_test_data\n",
    "train_data = scale_graphs(train_data, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    "val_data = scale_graphs(val_data, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    "test_data = scale_graphs(test_data, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    "#Load the data into DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARITY PLOT GENERATION\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import torch\n",
    "\n",
    "def collect_predictions_and_true_values(model, data_loader, device):\n",
    "    predictions = []\n",
    "    true_values = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            predictions.extend(output.cpu().numpy())\n",
    "            true_values.extend(data.aco2.cpu().numpy())\n",
    "    \n",
    "    return predictions, true_values\n",
    "\n",
    "# Function to calculate RÂ² and RMSE\n",
    "def calculate_metrics(true_values, predictions):\n",
    "    r2 = r2_score(true_values, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predictions))\n",
    "    return r2, rmse\n",
    "\n",
    "# Function to save metrics to CSV\n",
    "def save_metrics_to_csv(r2_train, rmse_train, r2_val, rmse_val, r2_test, rmse_test, parent_directory):\n",
    "    # Create the metrics dictionary\n",
    "    metrics_data = {\n",
    "        'Dataset': ['Training', 'Validation', 'Test'],\n",
    "        'R2': [r2_train, r2_val, r2_test],\n",
    "        'RMSE': [rmse_train, rmse_val, rmse_test]\n",
    "    }\n",
    "\n",
    "# Function to plot the parity plot with marginal histograms\n",
    "def plot_parity_plot(train_true_values, train_predictions, \n",
    "                     val_true_values, val_predictions, \n",
    "                     test_true_values, test_predictions,\n",
    "                     parent_directory=None):\n",
    "    fontsize = 16\n",
    "    matplotlib.rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "    # Calculate metrics\n",
    "    r2_train, rmse_train = calculate_metrics(train_true_values, train_predictions)\n",
    "    r2_val, rmse_val = calculate_metrics(val_true_values, val_predictions)\n",
    "    r2_test, rmse_test = calculate_metrics(test_true_values, test_predictions)\n",
    "\n",
    "    # Create figure with gridspec for histograms\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    gs = gridspec.GridSpec(2, 2, width_ratios=[4, 1], height_ratios=[1, 4], \n",
    "                          hspace=0.00, wspace=0.00)\n",
    "    \n",
    "    # Main plot\n",
    "    ax = fig.add_subplot(gs[1, 0])\n",
    "    ax_histx = fig.add_subplot(gs[0, 0], sharex=ax)\n",
    "    ax_histy = fig.add_subplot(gs[1, 1], sharey=ax)\n",
    "\n",
    "    # Scatter plots (keeping your exact style)\n",
    "    ax.scatter(train_true_values, train_predictions, \n",
    "                edgecolors='b', alpha=0.5, c='b', marker='o', \n",
    "                label=f'Train   (RÂ² = {r2_train:.4f},  RMSE = {rmse_train:.4f})')\n",
    "    ax.scatter(val_true_values, val_predictions, \n",
    "                edgecolors='g', alpha=0.5, c='g', marker='^', \n",
    "                label=f'Val      (RÂ² = {r2_val:.4f},  RMSE = {rmse_val:.4f})')\n",
    "    ax.scatter(test_true_values, test_predictions, \n",
    "                edgecolors='r', alpha=0.5, c='r', marker='v', \n",
    "                label=f'Test     (RÂ² = {r2_test:.4f},  RMSE = {rmse_test:.4f})')\n",
    "\n",
    "    # Parity line (keeping your exact style)\n",
    "    max_val = max(max(train_true_values), max(val_true_values), max(test_true_values))\n",
    "    ax.plot([-0.1, max_val+0.5], [-0.1, max_val+0.5], '--', linewidth=1.5, color='black')\n",
    "\n",
    "    # Labels & ticks (keeping your exact formatting)\n",
    "    ax.set_xlabel('Actual Solubility', fontsize=fontsize)\n",
    "    ax.set_ylabel('Predicted Solubility', fontsize=fontsize)\n",
    "    ax.set_xlim(-0.1, 2.5)\n",
    "    ax.set_ylim(-0.1, 2.5)\n",
    "    ax.tick_params(axis='both', which='major', length=6, width=0.8, labelsize=fontsize)\n",
    "    ax.tick_params(axis='both', which='minor', length=4, width=0.8)\n",
    "    ax.minorticks_on()\n",
    "    ax.legend(fontsize=fontsize-3, loc='upper left', frameon=False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)    \n",
    "\n",
    "    # Add histograms with dataset differentiation\n",
    "    bins = np.linspace(-0.1, 2.5, 27)\n",
    "    \n",
    "    # Top histogram (experimental values) - stacked by dataset\n",
    "    ax_histx.hist([np.array(train_true_values).flatten(), \n",
    "                   np.array(val_true_values).flatten(), \n",
    "                   np.array(test_true_values).flatten()], \n",
    "                  bins=bins, color=['b', 'g', 'r'], \n",
    "                  alpha=0.5, stacked=True, edgecolor='black', linewidth=0.5)\n",
    "    ax_histx.tick_params(labelbottom=False, labelleft=False, left=False)\n",
    "    ax_histx.spines['top'].set_visible(False)\n",
    "    ax_histx.spines['right'].set_visible(False)\n",
    "    ax_histx.spines['left'].set_visible(False)\n",
    "    #ax_histx.spines['bottom'].set_visible(False)\n",
    "\n",
    "    # Right histogram (predicted values) - stacked by dataset\n",
    "    ax_histy.hist([np.array(train_predictions).flatten(), \n",
    "                   np.array(val_predictions).flatten(), \n",
    "                   np.array(test_predictions).flatten()], \n",
    "                  bins=bins, orientation='horizontal', color=['b', 'g', 'r'], \n",
    "                  alpha=0.5, stacked=True, edgecolor='black', linewidth=0.5)\n",
    "    ax_histy.tick_params(labelbottom=False, labelleft=False, bottom=False)\n",
    "    ax_histy.spines['top'].set_visible(False)\n",
    "    ax_histy.spines['right'].set_visible(False)\n",
    "    #ax_histy.spines['left'].set_visible(False)\n",
    "    ax_histy.spines['bottom'].set_visible(False)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Save metrics if needed\n",
    "    if parent_directory:\n",
    "        save_metrics_to_csv(r2_train, rmse_train, r2_val, rmse_val, r2_test, rmse_test, parent_directory)\n",
    "\n",
    "\n",
    "# Collect predictions and true values for training, validation, and test data\n",
    "train_predictions, train_true_values = collect_predictions_and_true_values(model, train_loader, device)\n",
    "val_predictions, val_true_values = collect_predictions_and_true_values(model, val_loader, device)\n",
    "test_predictions, test_true_values = collect_predictions_and_true_values(model, test_loader, device)\n",
    "\n",
    "# Plot the parity plot\n",
    "plot_parity_plot(train_true_values, train_predictions, \n",
    "                 val_true_values, val_predictions, \n",
    "                 test_true_values, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting embeddings\n",
    "unique_named_graphs = {}\n",
    "for graph in test_data:\n",
    "    name = graph['name']\n",
    "    if name not in unique_named_graphs:\n",
    "        unique_named_graphs[name] = graph  # Keep the first occurrence\n",
    "\n",
    "# Convert to a list\n",
    "unique_graph_list = list(unique_named_graphs.values())\n",
    "unique_graph_list\n",
    "\n",
    "unique_list = DataLoader(unique_graph_list, batch_size=batch_size, shuffle=False)\n",
    "graph_representations = []\n",
    "with torch.no_grad():\n",
    "    for data in unique_list:\n",
    "        data = data.to(device)\n",
    "        output = model(data)\n",
    "        graph_rep = model(data, extract_embeddings=True)\n",
    "        graph_representations.append(graph_rep.cpu().numpy())\n",
    "graph_representations_df = pd.DataFrame(np.vstack(graph_representations))\n",
    "graph_representations_df.columns = [f'dim_{i}' for i in range(graph_representations_df.shape[1])]\n",
    "embeddings_df = graph_representations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD & GRAPH GENERATION FOR EITHER SRS OR RAS\n",
    "df_components = load_dataset(get_path(file_name = 'components_set.csv', folder_name='datasets'))\n",
    "smiles_dict = dict(zip(df_components['Abbreviation'], df_components['SMILES']))\n",
    "df_systems = load_dataset(get_path(file_name = 'systems_set.csv', folder_name='datasets'))\n",
    "smiles_list = df_components[\"SMILES\"].dropna().tolist()\n",
    "mol_name_dict = smiles_dict.copy()\n",
    "# GRAPH\n",
    "system_graphs = process_dataset(df_systems, smiles_dict)\n",
    "# LOAD DATASET\n",
    "splitter = DataSplitter(system_graphs, random_state=split_seed)\n",
    "splitter.print_dataset_stats()\n",
    "# Options: rarity_aware_unseen_amine_split stratified_random_split\n",
    "train_data, val_data, test_data = splitter.rarity_aware_unseen_amine_split()\n",
    "#Retrieve the statistics of train_data\n",
    "stats = compute_statistics(train_data)\n",
    "conc_mean = stats[0]\n",
    "conc_std = stats[1]\n",
    "temp_mean = stats[2]\n",
    "temp_std = stats[3]\n",
    "pco2_mean = stats[4]\n",
    "pco2_std = stats[5]\n",
    "#Apply the scaling to validation and test\n",
    "original_train_data = copy.deepcopy(train_data)\n",
    "original_val_data = copy.deepcopy(val_data)\n",
    "original_test_data = copy.deepcopy(test_data)\n",
    "combined_original_data = original_train_data + original_val_data + original_test_data\n",
    "train_data = scale_graphs(train_data, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    "val_data = scale_graphs(val_data, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    "test_data = scale_graphs(test_data, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    "#Load the data into DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # LOAD DATASET FOR RASHYB\n",
    "df_components = load_dataset(get_path(file_name = 'components_set.csv', folder_name='datasets'))\n",
    "smiles_dict = dict(zip(df_components['Abbreviation'], df_components['SMILES']))\n",
    "df_systems = load_dataset(get_path(file_name = 'systems_set.csv', folder_name='datasets'))\n",
    "smiles_list = df_components[\"SMILES\"].dropna().tolist()\n",
    "mol_name_dict = smiles_dict.copy()\n",
    "# GRAPH\n",
    "system_graphs = process_dataset(df_systems, smiles_dict)\n",
    "splitter_1 = DataSplitter(system_graphs, random_state=split_seed)\n",
    "RASset1, RASset2, RASset3 = splitter_1.rarity_aware_unseen_amine_split()\n",
    "opt_data = RASset1 + RASset2\n",
    "\n",
    "# HYBRID\n",
    "splitter_2 = DataSplitter(opt_data, random_state=split_seed)\n",
    "SRSset1, SRSset2, SRSset3 = splitter_2.stratified_random_split()\n",
    "train_data = SRSset1\n",
    "val_data = SRSset2 + SRSset3\n",
    "test_data = RASset3\n",
    "#Retrieve the statistics of train_data\n",
    "stats = compute_statistics(train_data)\n",
    "conc_mean = stats[0]\n",
    "conc_std = stats[1]\n",
    "temp_mean = stats[2]\n",
    "temp_std = stats[3]\n",
    "pco2_mean = stats[4]\n",
    "pco2_std = stats[5]\n",
    "#Apply the scaling to validation and test\n",
    "original_train_data = copy.deepcopy(train_data)\n",
    "original_val_data = copy.deepcopy(val_data)\n",
    "original_test_data = copy.deepcopy(test_data)\n",
    "combined_original_data = original_train_data + original_val_data + original_test_data\n",
    "train_data = scale_graphs(train_data, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    "val_data = scale_graphs(val_data, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    "test_data = scale_graphs(test_data, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    "#Load the data into DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "baseline_dir = os.path.join(\n",
    "    os.path.dirname(os.getcwd()),\n",
    "    \"models\",\n",
    "    \"models_root\",\n",
    "    \"model_for_inference\",\n",
    "    \"ras_baseline\"\n",
    ")\n",
    "files = sorted(os.listdir(baseline_dir))\n",
    "if len(files) == 0:\n",
    "    raise FileNotFoundError(f\"No files found in {baseline_dir}\")\n",
    "model_file = files[0]\n",
    "print(f\"Using model file: {model_file}\")\n",
    "path = os.path.join(baseline_dir, model_file)\n",
    "selected_device = 'cuda'\n",
    "model_1 = load_model_for_inference(path, device=device)\n",
    "\n",
    "# PIGNN\n",
    "pinn_dir = os.path.join(\n",
    "    os.path.dirname(os.getcwd()),\n",
    "    \"models\",\n",
    "    \"models_root\",\n",
    "    \"model_for_inference\",\n",
    "    \"ras_pinn\"\n",
    ")\n",
    "files = sorted(os.listdir(pinn_dir))\n",
    "if len(files) == 0:\n",
    "    raise FileNotFoundError(f\"No files found in {pinn_dir}\")\n",
    "model_file = files[0]\n",
    "print(f\"Using model file: {model_file}\")\n",
    "path = os.path.join(pinn_dir, model_file)\n",
    "selected_device = 'cuda'\n",
    "model_2 = load_model_for_inference(path, device=device)\n",
    "\n",
    "# Load both models and prepare for comparison\n",
    "models = {\n",
    "    \"Baseline\": (model_1, lambda g: scale_graphs(g, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)),\n",
    "    \"PINN\": (model_2, lambda g: scale_graphs(g, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AAD CALCULATION\n",
    "models = {\n",
    "    \"Baseline\": (model_1, lambda g: scale_graphs(g, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)),\n",
    "    \"PINN\": (model_2, lambda g: scale_graphs(g, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)),\n",
    "}\n",
    "\n",
    "# Min points per temperature (filtering)\n",
    "min_points_per_temp = 2\n",
    "\n",
    "used_data_for_inference = original_test_data\n",
    "# Extract unique values from the data\n",
    "unique_systems = list(set(graph.name for graph in used_data_for_inference))\n",
    "unique_concentrations = list(set(graph.conc for graph in used_data_for_inference))\n",
    "unique_temperatures = list(set(graph.temp for graph in used_data_for_inference))\n",
    "unique_references = list(set(graph.ref for graph in used_data_for_inference))\n",
    "# Store AAD results\n",
    "aad_results = {model_name: {} for model_name in models.keys()}\n",
    "\n",
    "for amine in unique_systems:\n",
    "    # Collect all graphs for this amine across all refs and concentrations\n",
    "    graphs_amine = [g for g in used_data_for_inference if g.name == amine]\n",
    "    \n",
    "    # Temperature filtering\n",
    "    temp_counts = {temp: sum(1 for g in graphs_amine if g.temp == temp) for temp in set(g.temp for g in graphs_amine)}\n",
    "    filtered_temps = sorted([t for t, count in temp_counts.items() if count >= min_points_per_temp])\n",
    "    \n",
    "    if not filtered_temps:\n",
    "        continue  # skip amines without enough points\n",
    "    \n",
    "    # Keep only graphs in filtered temperatures\n",
    "    graphs_filtered = [g for g in graphs_amine if g.temp in filtered_temps]\n",
    "    \n",
    "    # For each model\n",
    "    for model_name, (model, scaler) in models.items():\n",
    "        abs_errors = []\n",
    "        for g in graphs_filtered:\n",
    "            g_pred = g.clone()\n",
    "            \n",
    "            # Ensure all scalar features are tensors\n",
    "            g_pred.temp = torch.tensor([g_pred.temp], dtype=torch.float)\n",
    "            g_pred.conc = torch.tensor([g_pred.conc], dtype=torch.float)\n",
    "            g_pred.pco2 = torch.tensor([g_pred.pco2], dtype=torch.float)\n",
    "            \n",
    "            g_pred = scaler(g_pred).to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(g_pred).cpu().numpy().flatten()\n",
    "            \n",
    "            abs_errors.append(np.abs(pred - g_pred.aco2))  # g_pred.aco2 can stay float\n",
    "            \n",
    "        # Average absolute deviation\n",
    "        aad_results[model_name][amine] = np.mean(abs_errors)\n",
    "\n",
    "# Print AAD per amine per model\n",
    "for model_name, amines in aad_results.items():\n",
    "    print(f\"\\n=== {model_name} AAD per amine ===\")\n",
    "    for amine, aad in amines.items():\n",
    "        print(f\"{amine}: {aad:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAPE CALCULATION\n",
    "models = {\n",
    "    \"Baseline\": (model_1, lambda g: scale_graphs(g, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)),\n",
    "    \"PINN\": (model_2, lambda g: scale_graphs(g, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)),\n",
    "}\n",
    "\n",
    "# Min points per temperature (filtering)\n",
    "min_points_per_temp = 2\n",
    "used_data_for_inference = original_test_data\n",
    "# Extract unique values from the data\n",
    "unique_systems = list(set(graph.name for graph in used_data_for_inference))\n",
    "unique_concentrations = list(set(graph.conc for graph in used_data_for_inference))\n",
    "unique_temperatures = list(set(graph.temp for graph in used_data_for_inference))\n",
    "unique_references = list(set(graph.ref for graph in used_data_for_inference))\n",
    "# Store MAPE results\n",
    "mape_results = {model_name: {} for model_name in models.keys()}\n",
    "\n",
    "for amine in unique_systems:\n",
    "    # Collect all graphs for this amine across all refs and concentrations\n",
    "    graphs_amine = [g for g in used_data_for_inference if g.name == amine]\n",
    "    \n",
    "    # Temperature filtering\n",
    "    temp_counts = {temp: sum(1 for g in graphs_amine if g.temp == temp) for temp in set(g.temp for g in graphs_amine)}\n",
    "    filtered_temps = sorted([t for t, count in temp_counts.items() if count >= min_points_per_temp])\n",
    "    \n",
    "    if not filtered_temps:\n",
    "        continue  # skip amines without enough points\n",
    "    \n",
    "    # Keep only graphs in filtered temperatures\n",
    "    graphs_filtered = [g for g in graphs_amine if g.temp in filtered_temps]\n",
    "    \n",
    "    # For each model\n",
    "    for model_name, (model, scaler) in models.items():\n",
    "        percentage_errors = []\n",
    "        for g in graphs_filtered:\n",
    "            g_pred = g.clone()\n",
    "            \n",
    "            # Ensure all scalar features are tensors\n",
    "            g_pred.temp = torch.tensor([g_pred.temp], dtype=torch.float)\n",
    "            g_pred.conc = torch.tensor([g_pred.conc], dtype=torch.float)\n",
    "            g_pred.pco2 = torch.tensor([g_pred.pco2], dtype=torch.float)\n",
    "            \n",
    "            g_pred = scaler(g_pred).to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(g_pred).cpu().numpy().flatten()\n",
    "            \n",
    "            # Avoid division by zero\n",
    "            if g_pred.aco2 != 0:\n",
    "                percentage_errors.append(np.abs(pred - g_pred.aco2) / np.abs(g_pred.aco2))\n",
    "        \n",
    "        # Mean Absolute Percentage Error\n",
    "        mape_results[model_name][amine] = np.mean(percentage_errors) * 100\n",
    "\n",
    "# Print MAPE per amine per model\n",
    "for model_name, amines in mape_results.items():\n",
    "    print(f\"\\n=== {model_name} MAPE per amine (%) ===\")\n",
    "    for amine, mape in amines.items():\n",
    "        print(f\"{amine}: {mape:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENSEMBLE PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD & GRAPH GENERATION FOR EITHER SRS OR RAS\n",
    "df_components = load_dataset(get_path(file_name = 'components_set.csv', folder_name='datasets'))\n",
    "smiles_dict = dict(zip(df_components['Abbreviation'], df_components['SMILES']))\n",
    "df_systems = load_dataset(get_path(file_name = 'systems_set.csv', folder_name='datasets'))\n",
    "smiles_list = df_components[\"SMILES\"].dropna().tolist()\n",
    "mol_name_dict = smiles_dict.copy()\n",
    "# GRAPH\n",
    "system_graphs = process_dataset(df_systems, smiles_dict)\n",
    "# LOAD DATASET\n",
    "splitter = DataSplitter(system_graphs, random_state=split_seed)\n",
    "splitter.print_dataset_stats()\n",
    "# Options: rarity_aware_unseen_amine_split stratified_random_split\n",
    "train_data, val_data, test_data = splitter.rarity_aware_unseen_amine_split()\n",
    "#Retrieve the statistics of train_data\n",
    "stats = compute_statistics(train_data)\n",
    "conc_mean = stats[0]\n",
    "conc_std = stats[1]\n",
    "temp_mean = stats[2]\n",
    "temp_std = stats[3]\n",
    "pco2_mean = stats[4]\n",
    "pco2_std = stats[5]\n",
    "#Apply the scaling to validation and test\n",
    "original_train_data = copy.deepcopy(train_data)\n",
    "original_val_data = copy.deepcopy(val_data)\n",
    "original_test_data = copy.deepcopy(test_data)\n",
    "combined_original_data = original_train_data + original_val_data + original_test_data\n",
    "train_data = scale_graphs(train_data, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    "val_data = scale_graphs(val_data, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    "test_data = scale_graphs(test_data, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    "#Load the data into DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "\n",
    "def load_ensemble_models(model_dir, device, scaler_fn):\n",
    "    \"\"\"\n",
    "    Load all .pth models in a directory and return a list of (model, scaler_fn).\n",
    "    \"\"\"\n",
    "    model_paths = sorted(glob.glob(f\"{model_dir}/*.pth\"))\n",
    "    models = []\n",
    "    for path in model_paths:\n",
    "        model = load_model_for_inference(path, device=device)\n",
    "        models.append((model, scaler_fn))\n",
    "        print(f\"Loaded model: {path}\")\n",
    "    return models\n",
    "# BASELINE\n",
    "model_dir_baseline = os.path.join(\n",
    "    os.path.dirname(os.getcwd()),\n",
    "    \"models\",\n",
    "    \"models_root\",\n",
    "    \"model_for_inference\",\n",
    "    \"ras_baseline\"\n",
    ")\n",
    "baseline_models = load_ensemble_models(\n",
    "    f\"{model_dir_baseline}\", \n",
    "    device,\n",
    "    lambda g: scale_graphs(g, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    ")\n",
    "# PIGNN\n",
    "model_dir_pinn = os.path.join(\n",
    "    os.path.dirname(os.getcwd()),\n",
    "    \"models\",\n",
    "    \"models_root\",\n",
    "    \"model_for_inference\",\n",
    "    \"ras_pinn\"\n",
    ")\n",
    "pinn_models = load_ensemble_models(\n",
    "    f\"{model_dir_pinn}\", \n",
    "    device,\n",
    "    lambda g: scale_graphs(g, conc_mean, conc_std, temp_mean, temp_std, pco2_mean, pco2_std)\n",
    ")\n",
    "# Wrap into a dict\n",
    "models = {\n",
    "    \"Baseline\": baseline_models,\n",
    "    \"PINN\": pinn_models,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AAD CALCULATION WITH ENSEMBLE\n",
    "# Min points per temperature (filtering)\n",
    "min_points_per_temp = 2\n",
    "\n",
    "used_data_for_inference = original_train_data\n",
    "\n",
    "# Extract unique values from the data\n",
    "unique_systems = list(set(graph.name for graph in used_data_for_inference))\n",
    "unique_concentrations = list(set(graph.conc for graph in used_data_for_inference))\n",
    "unique_temperatures = list(set(graph.temp for graph in used_data_for_inference))\n",
    "unique_references = list(set(graph.ref for graph in used_data_for_inference))\n",
    "\n",
    "# Store AAD results\n",
    "aad_results = {model_name: {} for model_name in models.keys()}\n",
    "\n",
    "def predict_with_ensemble(model_list, graph, device):\n",
    "    \"\"\"\n",
    "    Run inference with an ensemble of models and return the averaged prediction.\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for model, scaler in model_list:\n",
    "        g_pred = graph.clone()\n",
    "\n",
    "        # Ensure all scalar features are tensors\n",
    "        g_pred.temp = torch.tensor([g_pred.temp], dtype=torch.float)\n",
    "        g_pred.conc = torch.tensor([g_pred.conc], dtype=torch.float)\n",
    "        g_pred.pco2 = torch.tensor([g_pred.pco2], dtype=torch.float)\n",
    "\n",
    "        g_pred = scaler(g_pred).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(g_pred).cpu().numpy().flatten()\n",
    "        preds.append(pred)\n",
    "\n",
    "    return np.mean(preds, axis=0)  # average across ensemble\n",
    "\n",
    "# === Main loop ===\n",
    "for amine in unique_systems:\n",
    "    # Collect all graphs for this amine across all refs and concentrations\n",
    "    graphs_amine = [g for g in used_data_for_inference if g.name == amine]\n",
    "    \n",
    "    # Temperature filtering\n",
    "    temp_counts = {temp: sum(1 for g in graphs_amine if g.temp == temp) for temp in set(g.temp for g in graphs_amine)}\n",
    "    filtered_temps = sorted([t for t, count in temp_counts.items() if count >= min_points_per_temp])\n",
    "    \n",
    "    if not filtered_temps:\n",
    "        continue  # skip amines without enough points\n",
    "    \n",
    "    # Keep only graphs in filtered temperatures\n",
    "    graphs_filtered = [g for g in graphs_amine if g.temp in filtered_temps]\n",
    "    \n",
    "    # For each model family (Baseline / PINN)\n",
    "    for model_name, model_list in models.items():\n",
    "        abs_errors = []\n",
    "        for g in graphs_filtered:\n",
    "            pred = predict_with_ensemble(model_list, g, device)\n",
    "            abs_errors.append(np.abs(pred - g.aco2))  # g.aco2 can stay float\n",
    "        \n",
    "        # Average absolute deviation\n",
    "        aad_results[model_name][amine] = np.mean(abs_errors)\n",
    "\n",
    "# === Print results ===\n",
    "for model_name, amines in aad_results.items():\n",
    "    print(f\"\\n=== {model_name} AAD per amine ===\")\n",
    "    for amine, aad in amines.items():\n",
    "        print(f\"{amine}: {aad:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAPE CALCULATION WITH ENSEMBLE\n",
    "# Min points per temperature (filtering)\n",
    "min_points_per_temp = 2\n",
    "used_data_for_inference = original_train_data\n",
    "\n",
    "# Extract unique values from the data\n",
    "unique_systems = list(set(graph.name for graph in used_data_for_inference))\n",
    "unique_concentrations = list(set(graph.conc for graph in used_data_for_inference))\n",
    "unique_temperatures = list(set(graph.temp for graph in used_data_for_inference))\n",
    "unique_references = list(set(graph.ref for graph in used_data_for_inference))\n",
    "\n",
    "# Store MAPE results\n",
    "mape_results = {model_name: {} for model_name in models.keys()}\n",
    "\n",
    "def predict_with_ensemble(model_list, graph, device):\n",
    "    \"\"\"\n",
    "    Run inference with an ensemble of models and return the averaged prediction.\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    for model, scaler in model_list:\n",
    "        g_pred = graph.clone()\n",
    "\n",
    "        # Ensure all scalar features are tensors\n",
    "        g_pred.temp = torch.tensor([g_pred.temp], dtype=torch.float)\n",
    "        g_pred.conc = torch.tensor([g_pred.conc], dtype=torch.float)\n",
    "        g_pred.pco2 = torch.tensor([g_pred.pco2], dtype=torch.float)\n",
    "\n",
    "        g_pred = scaler(g_pred).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(g_pred).cpu().numpy().flatten()\n",
    "        preds.append(pred)\n",
    "\n",
    "    return np.mean(preds, axis=0)  # average across ensemble\n",
    "\n",
    "# === Main loop ===\n",
    "for amine in unique_systems:\n",
    "    # Collect all graphs for this amine across all refs and concentrations\n",
    "    graphs_amine = [g for g in used_data_for_inference if g.name == amine]\n",
    "    \n",
    "    # Temperature filtering\n",
    "    temp_counts = {temp: sum(1 for g in graphs_amine if g.temp == temp) for temp in set(g.temp for g in graphs_amine)}\n",
    "    filtered_temps = sorted([t for t, count in temp_counts.items() if count >= min_points_per_temp])\n",
    "    \n",
    "    if not filtered_temps:\n",
    "        continue  # skip amines without enough points\n",
    "    \n",
    "    # Keep only graphs in filtered temperatures\n",
    "    graphs_filtered = [g for g in graphs_amine if g.temp in filtered_temps]\n",
    "    \n",
    "    # For each model family (Baseline / PINN)\n",
    "    for model_name, model_list in models.items():\n",
    "        percentage_errors = []\n",
    "        for g in graphs_filtered:\n",
    "            pred = predict_with_ensemble(model_list, g, device)\n",
    "\n",
    "            # Avoid division by zero\n",
    "            if g.aco2 != 0:\n",
    "                percentage_errors.append(np.abs(pred - g.aco2) / np.abs(g.aco2))\n",
    "        \n",
    "        # Mean Absolute Percentage Error\n",
    "        mape_results[model_name][amine] = np.mean(percentage_errors) * 100\n",
    "\n",
    "# === Print results ===\n",
    "for model_name, amines in mape_results.items():\n",
    "    print(f\"\\n=== {model_name} MAPE per amine (%) ===\")\n",
    "    for amine, mape in amines.items():\n",
    "        print(f\"{amine}: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize original_test_data to see available combinations for isotherm visualization\n",
    "# Get unique graph\n",
    "unique_named_graphs = {}\n",
    "for graph in test_data:\n",
    "    name = graph['name']\n",
    "    if name not in unique_named_graphs:\n",
    "        unique_named_graphs[name] = graph  # Keep the first occurrence\n",
    "\n",
    "# Optional: convert to a list\n",
    "unique_graph_list = list(unique_named_graphs.values())\n",
    "unique_list = DataLoader(unique_graph_list, batch_size=batch_size, shuffle=False)\n",
    "data_summary = []\n",
    "for point in original_test_data:\n",
    "    data_summary.append({\n",
    "        'name': point['name'],\n",
    "        'temp': point.temp,\n",
    "        'conc': point.conc,\n",
    "        'pco2': point.pco2,\n",
    "        'aco2': point.aco2\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "df_summary = pd.DataFrame(data_summary)\n",
    "\n",
    "# Group by amine to see available conditions\n",
    "print(\"Available experimental conditions by amine:\")\n",
    "for amine in df_summary['name'].unique():\n",
    "    amine_data = df_summary[df_summary['name'] == amine]\n",
    "    print(f\"\\n{amine}:\")\n",
    "    \n",
    "    # List unique temperatures\n",
    "    unique_temps = sorted(amine_data['temp'].unique())\n",
    "    print(f\"  Temperatures (K): {[float(t) for t in unique_temps]}\")\n",
    "    \n",
    "    # List unique concentrations\n",
    "    unique_concs = sorted(amine_data['conc'].unique())\n",
    "    print(f\"  Concentrations (M): {[float(c) for c in unique_concs]}\")\n",
    "    \n",
    "    print(f\"  pCO2 range: {amine_data['pco2'].min():.2e} kPa - {amine_data['pco2'].max():.2e} kPa\")\n",
    "    print(f\"  Total data points: {len(amine_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose specific conditions based on data availability\n",
    "# Simply type the amine name you want to analyze\n",
    "target_amine = \"1DMA2P\"  # Replace with the amine name from the list above\n",
    "amine_data = df_summary[df_summary['name'] == target_amine]\n",
    "selected_temp = 313.15  # Replace with desired temperature from the list\n",
    "selected_conc = 2    # Replace with desired concentration from the list\n",
    "\n",
    "# Show all available combinations for this amine\n",
    "print(f\"\\nAll available temperature-concentration combinations for {target_amine}:\")\n",
    "temp_conc_combinations = amine_data[['temp', 'conc']].drop_duplicates().sort_values(['temp', 'conc'])\n",
    "for idx, row in temp_conc_combinations.iterrows():\n",
    "   count = len(amine_data[(amine_data['temp'] == row['temp']) & (amine_data['conc'] == row['conc'])])\n",
    "   print(f\"  T={float(row['temp'])} K, C={float(row['conc'])} M ({count} data points)\")\n",
    "\n",
    "# Filter experimental data for these exact conditions\n",
    "exp_points = amine_data[\n",
    "   (amine_data['temp'] == selected_temp) & \n",
    "   (amine_data['conc'] == selected_conc)\n",
    "]\n",
    "\n",
    "print(f\"\\nExperimental points for {target_amine} at T={selected_temp} K, C={selected_conc} M: {len(exp_points)}\")\n",
    "if len(exp_points) > 0:\n",
    "   print(\"pCO2 values:\")\n",
    "   print(f\"  {[float(p) for p in sorted(exp_points['pco2'].unique())]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use the conditions determined from data organization above\n",
    "temp = selected_temp  # From your previous analysis\n",
    "conc = selected_conc  # From your previous analysis\n",
    "\n",
    "# Create pCO2 grid based on experimental data range + extra coverage\n",
    "if len(exp_points) > 0:\n",
    "    exp_pco2_min = exp_points['pco2'].min()\n",
    "    exp_pco2_max = exp_points['pco2'].max()\n",
    "    \n",
    "    # Extend range by factor by 20% on each side\n",
    "    pco2_min = exp_pco2_min * 0.8\n",
    "    pco2_max = exp_pco2_max * 1.2\n",
    "    \n",
    "    # Create log-spaced grid that includes experimental points\n",
    "    pco2_values = np.logspace(np.log10(pco2_min), np.log10(pco2_max), 200)\n",
    "    \n",
    "    print(f\"Experimental pCO2 range: {exp_pco2_min:.2e} - {exp_pco2_max:.2e}\")\n",
    "    print(f\"Prediction pCO2 range: {pco2_min:.2e} - {pco2_max:.2e}\")\n",
    "else:\n",
    "    # Fallback to default range if no experimental data\n",
    "    pco2_values = np.logspace(-2, 4, 200)\n",
    "    print(\"No experimental data - using default pCO2 range: 1e-2 to 1e4\")\n",
    "\n",
    "for model_name, model_list in models.items():  # models = {\"Baseline\": [...], \"PINN\": [...]}\n",
    "    all_preds = []\n",
    "\n",
    "    # Use the already filtered experimental points from above\n",
    "    current_amine = target_amine\n",
    "    \n",
    "    # Extract experimental pco2 and aco2 values (already in real scale)\n",
    "    exp_pco2 = exp_points['pco2'].tolist()\n",
    "    exp_aco2 = exp_points['aco2'].tolist()\n",
    "\n",
    "    # Collect predictions from all models in the ensemble\n",
    "    for model, scaler in model_list:\n",
    "        preds = []\n",
    "        for pco2 in pco2_values:\n",
    "            # Find the graph object for current_amine\n",
    "            graph = next(g for g in unique_graph_list if g['name'] == current_amine)\n",
    "            g = graph.clone()\n",
    "            g.temp = torch.tensor([temp], dtype=torch.float)\n",
    "            g.conc = torch.tensor([conc], dtype=torch.float)\n",
    "            g.pco2 = torch.tensor([pco2], dtype=torch.float)\n",
    "            g = scaler(g).to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(g).cpu().numpy().flatten()\n",
    "            preds.append(pred)\n",
    "        all_preds.append(np.array(preds).flatten())\n",
    "\n",
    "    all_preds = np.array(all_preds)  # shape = (num_models, num_points)\n",
    "    mean_pred = all_preds.mean(axis=0)\n",
    "    std_pred = all_preds.std(axis=0)\n",
    "\n",
    "    # Plot with experimental data\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(pco2_values, mean_pred, color='blue', label=f'{model_name} Mean')\n",
    "    plt.fill_between(pco2_values, mean_pred - std_pred, mean_pred + std_pred,\n",
    "                     color='blue', alpha=0.3, label=f'Â±1 std')\n",
    "    \n",
    "    # Add experimental points\n",
    "    if exp_pco2:\n",
    "        plt.scatter(exp_pco2, exp_aco2, color='red', s=50, zorder=5, \n",
    "                   label='Experimental data', edgecolors='black', linewidth=0.5)\n",
    "    \n",
    "    plt.xlabel('pCO2')\n",
    "    plt.ylabel('Î±CO2')\n",
    "    plt.xscale('log')\n",
    "    plt.xlim(pco2_min if 'pco2_min' in locals() else 1e-2, \n",
    "             pco2_max if 'pco2_max' in locals() else 1e4)\n",
    "    plt.title(f'{current_amine}, {conc:.1f} M, {temp:.1f} K - {model_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"{model_name} ensemble mean Â±1 std calculated from {len(model_list)} models\")\n",
    "    if exp_pco2:\n",
    "        print(f\"Experimental points plotted: {len(exp_pco2)} at {temp:.1f}K, {conc:.1f}M\")\n",
    "    else:\n",
    "        print(f\"No experimental data found for {current_amine} at these conditions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use the conditions determined from data organization above\n",
    "temp = selected_temp  # From your previous analysis\n",
    "conc = selected_conc  # From your previous analysis\n",
    "\n",
    "# Create pCO2 grid based on experimental data range + extra coverage\n",
    "if len(exp_points) > 0:\n",
    "    exp_pco2_min = exp_points['pco2'].min()\n",
    "    exp_pco2_max = exp_points['pco2'].max()\n",
    "    \n",
    "    # Extend range by factor by 20% on each side\n",
    "    pco2_min = exp_pco2_min * 0.8\n",
    "    pco2_max = exp_pco2_max * 1.2\n",
    "    \n",
    "    # Create log-spaced grid that includes experimental points\n",
    "    pco2_values = np.linspace(0, 200, 200)\n",
    "    \n",
    "    print(f\"Experimental pCO2 range: {exp_pco2_min:.2e} - {exp_pco2_max:.2e}\")\n",
    "    print(f\"Prediction pCO2 range: {pco2_min:.2e} - {pco2_max:.2e}\")\n",
    "else:\n",
    "    # Fallback to default range if no experimental data\n",
    "    pco2_values = np.logspace(-2, 4, 200)\n",
    "    print(\"No experimental data - using default pCO2 range: 1e-2 to 1e4\")\n",
    "\n",
    "# Extract experimental data once\n",
    "current_amine = target_amine\n",
    "exp_pco2 = exp_points['pco2'].tolist() if len(exp_points) > 0 else []\n",
    "exp_aco2 = exp_points['aco2'].tolist() if len(exp_points) > 0 else []\n",
    "\n",
    "# Create single figure for both models\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Define colors for each model type\n",
    "colors = {'Baseline': 'blue', 'PINN': 'red'}\n",
    "\n",
    "# Process both model types\n",
    "model_predictions = {}\n",
    "\n",
    "for model_name, model_list in models.items():  # models = {\"Baseline\": [...], \"PINN\": [...]}\n",
    "    all_preds = []\n",
    "\n",
    "    # Collect predictions from all models in the ensemble\n",
    "    for model, scaler in model_list:\n",
    "        preds = []\n",
    "        for pco2 in pco2_values:\n",
    "            # Find the graph object for current_amine\n",
    "            graph = next(g for g in unique_graph_list if g['name'] == current_amine)\n",
    "            g = graph.clone()\n",
    "            g.temp = torch.tensor([temp], dtype=torch.float)\n",
    "            g.conc = torch.tensor([conc], dtype=torch.float)\n",
    "            g.pco2 = torch.tensor([pco2], dtype=torch.float)\n",
    "            g = scaler(g).to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(g).cpu().numpy().flatten()\n",
    "            preds.append(pred)\n",
    "        all_preds.append(np.array(preds).flatten())\n",
    "\n",
    "    all_preds = np.array(all_preds)  # shape = (num_models, num_points)\n",
    "    mean_pred = all_preds.mean(axis=0)\n",
    "    std_pred = all_preds.std(axis=0)\n",
    "    \n",
    "    # Store predictions for this model type\n",
    "    model_predictions[model_name] = {\n",
    "        'mean': mean_pred,\n",
    "        'std': std_pred,\n",
    "        'num_models': len(model_list)\n",
    "    }\n",
    "    \n",
    "    # Plot mean prediction line\n",
    "    color = colors.get(model_name, 'black')\n",
    "    plt.plot(pco2_values, mean_pred, color=color, label=f'{model_name} Mean', linewidth=2)\n",
    "    \n",
    "    # Plot uncertainty band\n",
    "    plt.fill_between(pco2_values, mean_pred - std_pred, mean_pred + std_pred,\n",
    "                     color=color, alpha=0.2, label=f'{model_name} Â±1 std')\n",
    "    \n",
    "    print(f\"{model_name} ensemble mean Â±1 std calculated from {len(model_list)} models\")\n",
    "\n",
    "# Add experimental points (only once, after both models are plotted)\n",
    "if exp_pco2:\n",
    "    plt.scatter(exp_pco2, exp_aco2, color='black', s=60, zorder=10, \n",
    "               label='Experimental data', edgecolors='white', linewidth=1)\n",
    "    print(f\"Experimental points plotted: {len(exp_pco2)} at {temp:.1f}K, {conc:.1f}M\")\n",
    "else:\n",
    "    print(f\"No experimental data found for {current_amine} at these conditions\")\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel('pCO2', fontsize=12)\n",
    "plt.ylabel('Î±CO2', fontsize=12)\n",
    "plt.xscale('linear')\n",
    "plt.xlim(0, \n",
    "         pco2_max if 'pco2_max' in locals() else 1e4)\n",
    "plt.title(f'{current_amine}, {conc:.1f} M, {temp:.1f} K - Model Comparison', fontsize=14)\n",
    "plt.legend(loc='best', fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for model_name, pred_data in model_predictions.items():\n",
    "    mean_values = pred_data['mean']\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  - Ensemble size: {pred_data['num_models']} models\")\n",
    "    print(f\"  - Î±CO2 range: {mean_values.min():.4f} - {mean_values.max():.4f}\")\n",
    "    print(f\"  - Mean uncertainty (std): {pred_data['std'].mean():.4f}\")\n",
    "\n",
    "# If both models exist, calculate and show differences\n",
    "if 'Baseline' in model_predictions and 'PINN' in model_predictions:\n",
    "    baseline_mean = model_predictions['Baseline']['mean']\n",
    "    pinn_mean = model_predictions['PINN']['mean']\n",
    "    \n",
    "    abs_diff = np.abs(baseline_mean - pinn_mean)\n",
    "    rel_diff = abs_diff / np.maximum(baseline_mean, 1e-10) * 100  # Avoid division by zero\n",
    "    \n",
    "    print(f\"\\nModel Differences:\")\n",
    "    print(f\"  - Maximum absolute difference: {abs_diff.max():.4f}\")\n",
    "    print(f\"  - Mean absolute difference: {abs_diff.mean():.4f}\")\n",
    "    print(f\"  - Maximum relative difference: {rel_diff.max():.2f}%\")\n",
    "    print(f\"  - Mean relative difference: {rel_diff.mean():.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
